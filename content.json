{"meta":{"title":"don't be evil","subtitle":"sakura ovq","description":"phper | golanger | sakuraovq","author":"sakura ovo","url":"https://www.sakuraus.cn"},"pages":[{"title":"404 Not Found _(:з」∠)_","date":"2018-11-26T13:16:31.664Z","updated":"2018-11-26T13:16:31.664Z","comments":true,"path":"/404.html","permalink":"https://www.sakuraus.cn//404.html","excerpt":"","text":""},{"title":"categories","date":"2018-08-05T07:57:10.000Z","updated":"2018-11-26T13:16:31.667Z","comments":false,"path":"categories/index.html","permalink":"https://www.sakuraus.cn/categories/index.html","excerpt":"","text":""},{"title":"","date":"2018-11-26T13:16:31.667Z","updated":"2018-11-26T13:16:31.667Z","comments":false,"path":"tags/index.html","permalink":"https://www.sakuraus.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"为你的server 安装上信号处理吧","slug":"signal","date":"2019-03-08T13:28:22.893Z","updated":"2019-03-08T13:28:22.805Z","comments":true,"path":"2019/03/08/signal/","link":"","permalink":"https://www.sakuraus.cn/2019/03/08/signal/","excerpt":"","text":"signal 信号 信号是 UNIX 操作系统特有的 IPC(进程间通信) 方式 很多服务端程序都是 后台运行的 那么我们有一天想要更新上线 如何通知到后台运行的程序呢那么发送信号是一种比较好的方式 1~15号信号为常用信号 用 kill 发起信号 我下面科普几种常用的 0=SIG_DFL 默认信号 使用场景: 通常用于 检测进程是否存在 2=SIGINT 这个信号通常是 Ctrl+C 发出的信号 使用场景: 在协程通信中 通常使用 channel 管道通信 在高并发情况下 channel 流动数据也是非常多 如果不小心 Ctrl+C 结束了允许中的server 那么正常情况 channel中未传递完的数据将被丢失,这种情况是我们不运行的 那么有什么解决方案呢 , 我们就可以为我们的服务 装上信号处理 监听退出信号 然后把channel中未传递完的方式可以放在 消息队列中服务,恢复在读取到 channel中 (这个来自360面试题解答) 9=SIGKILL 强制结束程序(危险) 通常我们是 用kill -9 pid 发出的 使用场景: 用来立即结束程序的运行.本信号不能被阻塞、处理和忽略。 如果管理员发现某个进程终止不了，可尝试发送这个信号。 10=SIGUSR1 留给用户的 使用场景: 我们可以监听这个信号 然后完成特定操作 比如 swoole 监听这个信号来重启 所有worker 12=SIGUSR2 留给用户的 使用场景: 我们可以监听这个信号 然后完成特定操作 比如 swoole 监听这个信号来重启 所有taskWorker 15=SIGTERM 程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理。 使用场景: 通常用来要求程序自己正常退出，这是一种柔性的关闭服务方式, shell命令kill缺省产生这个信号。如果进程终止不了，我们才会尝试SIGKILL。 php 处理 Unix 信号 下面介绍一种使用 pcntl 进程扩展 监听信号 实现的两种方式 PHP官方的pcntl_signal性能极差 很多纯PHP开发的后端框架中都使用了pcntl扩展提供的信号处理函数pcntl_signal，实际上这个函数的性能是很差的。首先看一段示例代码：12345678910111213141516171819&lt;?php declare(ticks = 1); pcntl_signal(SIGINT, 'signalHandler', false); /** * 处理信号 * @param int $sig 信号 */ function signalHandler($sig) &#123; switch ($sig) &#123; case SIGINT: echo \"\\r\\n\" . ' Ctrl+C 退出了'; exit; break; default: // 处理所有其他信号 &#125; &#125; 这段代码在执行pcntl_signal前，先加入了declare(ticks = 1)。因为PHP的函数无法直接注册到操作系统信号设置中，所以pcntl信号需要依赖tick机制。通过查看pcntl.c的源码实现发现。pcntl_signal的实现原理是，触发信号后先将信号加入一个队列中。然后在PHP的ticks回调函数中不断检查是否有信号，如果有信号就执行PHP中指定的回调函数，如果没有则跳出函数。下面是C 的代码12345678PHP_MINIT_FUNCTION(pcntl)&#123; php_register_signal_constants(INIT_FUNC_ARGS_PASSTHRU); php_pcntl_register_errno_constants(INIT_FUNC_ARGS_PASSTHRU); php_add_tick_function(pcntl_signal_dispatch TSRMLS_CC); return SUCCESS;&#125; 我们先看看pcntl_signal_dispatch() 怎么实现的 123456789101112131415161718192021void pcntl_signal_dispatch()&#123; //.... 这里略去一部分代码，queue即是信号队列 while (queue) &#123; if ((handle = zend_hash_index_find(&amp;PCNTL_G(php_signal_table), queue-&gt;signo)) != NULL) &#123; ZVAL_NULL(&amp;retval); ZVAL_LONG(&amp;param, queue-&gt;signo); /* Call php signal handler - Note that we do not report errors, and we ignore the return value */ /* FIXME: this is probably broken when multiple signals are handled in this while loop (retval) */ /* 调用用户注册的回调函数 */ call_user_function(EG(function_table), NULL, handle, &amp;retval, 1, &amp;param TSRMLS_CC); zval_ptr_dtor(&amp;param); zval_ptr_dtor(&amp;retval); &#125; next = queue-&gt;next; queue-&gt;next = PCNTL_G(spares); PCNTL_G(spares) = queue; queue = next; &#125;&#125; 这样就存在一个比较严重的性能问题，大家都知道PHP的ticks=1表示每执行1行PHP代码就回调此函数。实际上大部分时间都没有信号产生，但ticks的函数一直会执行。如果一个服务器程序1秒中接收1000次请求，平均每个请求要执行1000行PHP代码。那么PHP的pcntl_signal，就带来了额外的 1000 * 1000，也就是100万次空的函数调用。这样会浪费大量的CPU资源。 比较好的做法是去掉ticks，转而使用pcntl_signal_dispatch，在代码循环中自行处理信号。我就直接上代码了 我们自己实现一个 ticks 利用pcntl_signal_dispatch调度 1234567891011121314151617181920212223242526272829303132333435&lt;?php pcntl_signal(SIGUSR1, 'signalHandler', false); // 10 自定义信号 $status = 0; while (true) &#123; // 当发现信号队列,一旦发现有信号就会触发进程绑定事件回调 pcntl_signal_dispatch(); // 当信号到达之后就会被中断 $pid = pcntl_wait($status); // to do something 可以监听 进程是否正常退出... 然后做后续维护处理 /** // $status 接收到的是 信号标识 if ($pid &gt; 0 &amp;&amp; $pid != $masterPid &amp;&amp; !pcntl_wifexited($status)) &#123; // 如果退出 重新拉起 $this-&gt;start(1); &#125; */ // 进程重启的过程当中会有新的信号过来,如果没有调用pcntl_signal_dispatch,信号不会被处理 做打断处理 // (解决并发信号) pcntl_signal_dispatch(); &#125; /** * 处理信号 * @param int $sig 信号 */ function signalHandler($sig) &#123; switch ($sig) &#123; case SIGUSR1: // TODO:: 重启worker echo \"重新起worker\"; break; &#125; &#125; swoole中因为底层是C实现的，信号处理不受PHP的影响。 有兴趣大家可以去看看 swoole使用了目前Linux系统中最先进的signalfd来处理信号，几乎是没有任何额外消耗的。 参考 PHP官方的pcntl_signal性能极差 下面附上基础的 master-worker 处理模型代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316&lt;?php/** * ab压测 ab -n 100000 -c 10000 -k http://127.0.0.1:8000 * 批量杀 ps -ef | grep Event.php | grep -v grep | awk '&#123;print $2&#125;' | xargs kill -9 */class Event&#123; const MESSAGE = 'message'; // 消息事件 const CONNECT = 'connect'; // 连接事件 const CLOSE = 'close'; // 连接关闭时间 const SHUTDOWN = 'shutdown'; // 服务终止事件 /** * master 进程ID * @var array */ public $masterPid; /** * worker count * @var int */ public $workerNum = 2; /** * 监听服务地址 * @var string */ public $severAddr = 'tcp://127.0.0.1:8000'; /** * 工作进程pid * @var array */ private $workerPids = []; /** * 绑定的事件 * @var array */ private $event = []; /** * Event constructor. * @param string $listenAddress 监听地址 * @param int $workerNum worker 个数 */ public function __construct(string $listenAddress, $workerNum = 2) &#123; $this-&gt;severAddr = $listenAddress; // 获取masterPid $this-&gt;masterPid = posix_getpid(); // worker num $this-&gt;workerNum = $workerNum; &#125; /** * 绑定事件 * @param string $eventType * @param callable $callback * @throws \\Exception */ public function on(string $eventType, callable $callback) &#123; switch ($eventType) &#123; case self::MESSAGE: // 消息触发事件 $this-&gt;event[self::MESSAGE] = $callback; break; case self::CONNECT: // 连接事件 $this-&gt;event[self::CONNECT] = $callback; break; case self::CLOSE: // 关闭连接事件 $this-&gt;event[self::CLOSE] = $callback; break; case self::SHUTDOWN: // 终止事件 $this-&gt;event[self::SHUTDOWN] = $callback; break; default: throw new \\InvalidArgumentException('undefined event type'); &#125; &#125; /** * 触发事件 * @param string $eventType * @param mixed ...$params * @return mixed|null */ public function trigger(string $eventType, ...$params) &#123; $result = null; if (isset($this-&gt;event[$eventType])) &#123; $result = call_user_func($this-&gt;event[$eventType], ...$params); &#125; return $result; &#125; /** * 启动服务 */ public function start() &#123; // fork worker $this-&gt;fork($this-&gt;workerNum); // 处理 linux 信号 $this-&gt;signalDispatch(); &#125; /** * fork worker * @param int $workerNum 启用worker个数 */ public function fork($workerNum) &#123; for ($i = 0; $i &lt; $workerNum; $i++) &#123; $pid = pcntl_fork(); // 创建失败 if ($pid == -1) &#123; $this-&gt;trigger(self::SHUTDOWN); die; &#125; if ($pid &gt; 0) &#123; //父进程空间，返回子进程id $this-&gt;workerPids[$pid] = $pid; &#125; // worker空间 if ($pid == 0) &#123; $this-&gt;ePoll(); // 一定要结束不然 会fork 嵌套 die; &#125; &#125; &#125; /** * 信号调度 * @param int $status */ private function signalDispatch($status = 0) &#123; // 注册信号事件回调,是不会自动执行的 pcntl_signal(SIGTERM, [$this, 'signalHandler'], false); // 15 终止信号 pcntl_signal(SIGINT, [$this, 'signalHandler'], false); // 2 Ctrl+C 信号 pcntl_signal(SIGUSR1, [$this, 'signalHandler'], false); // 10 自定义信号(重启所有worker) while (true) &#123; // 当发现信号队列,一旦发现有信号就会触发进程绑定事件回调 pcntl_signal_dispatch(); // 当信号到达之后就会被中断 $pid = pcntl_wait($status); /** // 检测 子进程是否异常退出 if ($pid &gt; 0 &amp;&amp; $pid != $this-&gt;masterPid &amp;&amp; !pcntl_wifexited($status)) &#123; // 如果退出 重新拉起 $this-&gt;fork(1); &#125; */ // 进程重启的过程当中会有新的信号过来,如果没有调用pcntl_signal_dispatch,信号不会被处理(解决并发信号) pcntl_signal_dispatch(); &#125; &#125; /** * 处理主进程信号 * @param int $sig 信号 */ private function signalHandler($sig) &#123; switch ($sig) &#123; case SIGUSR1: // reload workers echo 'worker reloading' . PHP_EOL; $this-&gt;reload(); break; case SIGTERM: // 15 终止信号 echo \"\\r\\n\" . ' 程序退出了'; $this-&gt;kill(); exit; break; case SIGINT: // 2 Ctrl+C //处理SIGHUP信号 Ctrl+C 退出信号 echo \"\\r\\n\" . ' Ctrl+C 退出了'; $this-&gt;kill(); exit; break; default: // 处理所有其他信号 &#125; &#125; /** * 重启 所有worker */ private function reload() &#123; $this-&gt;kill(); // 在启动新的worker $this-&gt;fork($this-&gt;workerNum); &#125; /** * 杀掉所有worker */ private function kill() &#123; // 先杀掉worker foreach ($this-&gt;workerPids as $pid) &#123; posix_kill($pid, SIGKILL); unset($this-&gt;workerPids[$pid]); &#125; &#125; /** * 创建事件监听 */ private function ePoll() &#123; // 创建资源上下文 环境 配置端口监听复用 $context = stream_context_create([ 'socket' =&gt; [ 'so_reuseport' =&gt; 1, // 采用端口复用监听,系统会负载均衡 分发给worker请求,还不会出现惊群现象 ], ]); $serverSocket = stream_socket_server($this-&gt;severAddr, $errNo, $errStr, STREAM_SERVER_BIND | STREAM_SERVER_LISTEN, $context); swoole_event_add($serverSocket, $this-&gt;listenCallback()); &#125; /** * 返回 服务端回调闭包 * @return Closure */ private function listenCallback() &#123; return function ($fd) &#123; $client = stream_socket_accept($fd); // 新的连接 $this-&gt;trigger(self::CONNECT, $client); $this-&gt;receive($client); &#125;; &#125; /** * 监听客户端socket 可读写 * @param $clientSocket */ private function receive($clientSocket) &#123; swoole_event_add($clientSocket, function ($fd) &#123; $data = $this-&gt;fRead($fd); $this-&gt;trigger(self::MESSAGE, $fd, $data); // 关闭客户端 $this-&gt;close($fd); &#125;); &#125; /** * 读取数据 * @param $fd * @param int $size * @return string */ private function fRead($fd, $size = 8192): string &#123; $data = fread($fd, $size); return $data; &#125; /** * 关闭client socket * @param $fd */ private function close($fd): void &#123; $this-&gt;trigger(self::CLOSE, $fd); // socket处理完成后，从epoll事件中移除socket swoole_event_del($fd); fclose($fd); &#125;&#125;$worker = new Event('tcp://127.0.0.1:8000', 2);$worker-&gt;on('message', function ($fd, $data) &#123; $content = \"hello world\"; $httpResponse = \"HTTP/1.1 200 OK\\r\\n\"; $httpResponse .= \"Content-Type: text/html;charset=UTF-8\\r\\n\"; $httpResponse .= \"Connection: keep-alive\\r\\n\"; //连接保持 $httpResponse .= \"Server: php socket server\\r\\n\"; $httpResponse .= \"Content-length: \" . strlen($content) . \"\\r\\n\\r\\n\"; $httpResponse .= $content; fwrite($fd, $httpResponse);&#125;);$worker-&gt;on('connect', function ($socket) &#123; //echo '新的连接' . $socket . PHP_EOL;&#125;);$worker-&gt;on('close', function ($socket) &#123; // echo '连接关闭' . $socket . PHP_EOL;&#125;);$worker-&gt;on('shutdown', function () &#123; echo \"监听异常退出\";&#125;);$worker-&gt;start();","categories":[{"name":"php","slug":"php","permalink":"https://www.sakuraus.cn/categories/php/"},{"name":"linux","slug":"php/linux","permalink":"https://www.sakuraus.cn/categories/php/linux/"}],"tags":[{"name":"signal","slug":"signal","permalink":"https://www.sakuraus.cn/tags/signal/"}]},{"title":"使用inotfy 扩展 让swoole热加载","slug":"inotfy","date":"2019-03-07T16:00:00.000Z","updated":"2019-03-08T09:14:58.832Z","comments":true,"path":"2019/03/08/inotfy/","link":"","permalink":"https://www.sakuraus.cn/2019/03/08/inotfy/","excerpt":"","text":"inotifyinotify 介绍 inotify是Linux内核提供的一组系统调用，它可以监控文件系统操作，比如文件或者目录的创建、读取、写入、权限修改和删除等。inotify使用也很简单，使用inotify_init创建一个句柄，然后通过inotify_add_watch/inotify_rm_watch增加/删除对文件和目录的监听。PHP中提供了inotify扩展，支持了inotify系统调用。inotify本身也是一个文件描述符，可以加入到事件循环中，配合使用swoole扩展，就可以异步非阻塞地实时监听文件/目录变化 php 安装 inotify 1、可以使用 pecl install inotify 2、（https://pecl.php.net/get/inotify-2.0.0.tgz ）编译安装步骤跟之前一样，自行安装 实战Watch.php123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166&lt;?phpnamespace Swoole\\ToolKit;class NotFound extends \\Exception&#123;&#125;class Watch&#123; /** * @var resource */ protected $inotify; protected $pid; protected $reloadFileTypes = array('.php' =&gt; true); protected $watchFiles = array(); protected $afterNSeconds = 10; /** * 正在reload */ protected $reloading = false; protected $events; /** * 根目录 * @var array */ protected $rootDirs = array(); function putLog($log) &#123; $_log = \"[\" . date('Y-m-d H:i:s') . \"]\\t\" . $log . \"\\n\"; echo $_log; &#125; /** * @param $serverPid * @throws NotFound */ function __construct($serverPid) &#123; $this-&gt;pid = $serverPid; if (posix_kill($serverPid, 0) === false) &#123; throw new NotFound(\"Process#$serverPid not found.\"); &#125; $this-&gt;inotify = inotify_init(); $this-&gt;events = IN_MODIFY | IN_DELETE | IN_CREATE | IN_MOVE; swoole_event_add($this-&gt;inotify, function ($ifd) &#123; $events = inotify_read($this-&gt;inotify); if (!$events) &#123; return; &#125; //var_dump($events); foreach ($events as $ev) &#123; if ($ev['mask'] == IN_IGNORED) &#123; continue; &#125; else if ($ev['mask'] == IN_CREATE or $ev['mask'] == IN_DELETE or $ev['mask'] == IN_MODIFY or $ev['mask'] == IN_MOVED_TO or $ev['mask'] == IN_MOVED_FROM) &#123; $fileType = strrchr($ev['name'], '.'); //非重启类型 if (!isset($this-&gt;reloadFileTypes[$fileType])) &#123; continue; &#125; &#125; //正在reload，不再接受任何事件，冻结10秒 if (!$this-&gt;reloading) &#123; $this-&gt;putLog(\"after 10 seconds reload the server\"); //有事件发生了，进行重启 swoole_timer_after($this-&gt;afterNSeconds * 1000, array($this, 'reload')); $this-&gt;reloading = true; &#125; &#125; &#125;); &#125; function reload() &#123; $this-&gt;putLog(\"reloading\"); //向主进程发送信号 posix_kill($this-&gt;pid, SIGUSR1); //清理所有监听 $this-&gt;clearWatch(); //重新监听 foreach ($this-&gt;rootDirs as $root) &#123; $this-&gt;watch($root); &#125; //继续进行reload $this-&gt;reloading = false; &#125; /** * 添加文件类型 * @param $type */ function addFileType($type) &#123; $type = trim($type, '.'); $this-&gt;reloadFileTypes['.' . $type] = true; &#125; /** * 添加事件 * @param $inotifyEvent */ function addEvent($inotifyEvent) &#123; $this-&gt;events |= $inotifyEvent; &#125; /** * 清理所有inotify监听 */ function clearWatch() &#123; foreach ($this-&gt;watchFiles as $wd) &#123; inotify_rm_watch($this-&gt;inotify, $wd); &#125; $this-&gt;watchFiles = array(); &#125; /** * @param $dir * @param bool $root * @return bool * @throws NotFound */ function watch($dir, $root = true) &#123; //目录不存在 if (!is_dir($dir)) &#123; throw new NotFound(\"[$dir] is not a directory.\"); &#125; //避免重复监听 if (isset($this-&gt;watchFiles[$dir])) &#123; return false; &#125; //根目录 if ($root) &#123; $this-&gt;rootDirs[] = $dir; &#125; $wd = inotify_add_watch($this-&gt;inotify, $dir, $this-&gt;events); $this-&gt;watchFiles[$dir] = $wd; $files = scandir($dir); foreach ($files as $f) &#123; if ($f == '.' or $f == '..') &#123; continue; &#125; $path = $dir . '/' . $f; //递归目录 if (is_dir($path)) &#123; $this-&gt;watch($path, false); &#125; //检测文件类型 $fileType = strrchr($f, '.'); if (isset($this-&gt;reloadFileTypes[$fileType])) &#123; $wd = inotify_add_watch($this-&gt;inotify, $path, $this-&gt;events); $this-&gt;watchFiles[$path] = $wd; &#125; &#125; return true; &#125; function run() &#123; swoole_event_wait(); &#125;&#125;","categories":[{"name":"php","slug":"php","permalink":"https://www.sakuraus.cn/categories/php/"},{"name":"swoole","slug":"php/swoole","permalink":"https://www.sakuraus.cn/categories/php/swoole/"}],"tags":[{"name":"inotfy","slug":"inotfy","permalink":"https://www.sakuraus.cn/tags/inotfy/"},{"name":"swoole","slug":"swoole","permalink":"https://www.sakuraus.cn/tags/swoole/"}]},{"title":"关于git clean 的用法","slug":"git_clean","date":"2019-02-22T16:00:00.000Z","updated":"2019-02-23T02:28:02.464Z","comments":true,"path":"2019/02/23/git_clean/","link":"","permalink":"https://www.sakuraus.cn/2019/02/23/git_clean/","excerpt":"","text":"git clean -n是一次clean的演习, 告诉你哪些文件会被删除. 记住他不会真正的删除文件, 只是一个提醒 git clean -f删除当前目录下所有没有track过的文件. 他不会删除.gitignore文件里面指定的文件夹和文件, 不管这些文件有没有被track过 git clean -df删除当前目录下没有被track过的文件和文件夹 回到最新提交的版本git reset –hard 经常合并父级分支 或者同级分支经常 容易出 checkout 检出失败的问题 警告: git clean 删除你所有无路径的文件/目录,不能撤消。有时只是 clean -f 没有帮助。 如果你有未跟踪的目录，-d选项也需要： git reset --hard HEAD git clean -f -d git pull git_checkout","categories":[{"name":"git","slug":"git","permalink":"https://www.sakuraus.cn/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://www.sakuraus.cn/tags/git/"}]},{"title":"浅入浅出Mysql Innodb Mvcc","slug":"Mysql-Mvcc","date":"2018-12-20T08:36:04.177Z","updated":"2018-12-20T08:36:04.177Z","comments":true,"path":"2018/12/20/Mysql-Mvcc/","link":"","permalink":"https://www.sakuraus.cn/2018/12/20/Mysql-Mvcc/","excerpt":"","text":"MVCC MVCC 翻译过来就是指 多版本控制 多版本控制 指的是一种提高并发的技术最高的数据库系统, 只有读与读之间可以并发,写与读,读与写,写与写都要阻塞.引入多版本之后,只有写与写之间会相互阻塞, 其他三种操作都可以并发,这样大大提升了Innodb并发度,在内部实现中，与Postgres在数据行上实现多版本不同，InnoDB是在undolog中实现的，通过undolog可以找回数据的历史版本。找回的数据历史版本可以提供给用户读(按照隔离级别的定义，有些读请求只能看到比较老的数据版本)，也可以在回滚的时候覆盖数据页上的数据。在InnoDB内部中，会记录一个全局的活跃读写事务数组，其主要用来判断事务的可见性。 并发与并行关于并发与并行 只有才多核处理器才能体现Erlang之父用一张图解释了并发与并行 图 1 并发是两个队列交替使用一台咖啡机，并行是两个队列同时使用两台咖啡机，如果串行，一个队列使用一台咖啡机，那么哪怕前面那个人便秘了去厕所呆半天，后面的人也只能死等着他回来才能去接咖啡，这效率无疑是最低的。一台咖啡机就等于计算机中的一核心CPU,单核CPU在计算机时间片调度中同一时刻只有一个任务在执行多核CPU自然能同时调度多个任务执行啦 ,一个进程可以被多个核心CPU调用这种情况 会产生并行 并发是不是一个线程，并行是多个线程？ 答：并发和并行都可以是很多个线程，就看这些线程能不能同时被（多个）cpu执行，如果可以就说明是并行，而并发是多个线程被（一个）cpu 轮流切换着执行。 总结下 并行是同一时间执行多个任务(多个根据CPU核数决定), 并发是多个任务在同时执行但是在一段时间内只有一个任务被执行,默认计算的系统调度是抢占方,所以进程/线程过多会导致系统会很卡 这种方式才传统编程语言中一般是写一个线程池管理 但是在线程概念提出之前还有一个东西叫协程 也叫用户级线程,也就是说操作系统不会调,要语言来实现调用Golang 就是典型的 扯远了之前简单的简述了并发与并行 &lt;高性能Mysql&gt;中对MVCC的部分介绍 MySQL的大多数事务型存储引擎实现的其实都不是简单的行级锁。基于提升并发性能的考虑, 它们一般都同时实现了多版本并发控制(MVCC)。不仅是MySQL, 包括Oracle,PostgreSQL等其他数据库系统也都实现了MVCC, 但各自的实现机制不尽相同, 因为MVCC没有一个统一的实现标准。 可以认为MVCC是行级锁的一个变种, 但是它在很多情况下避免了加锁操作, 因此开销更低。虽然实现机制有所不同, 但大都实现了非阻塞的读操作，写操作也只锁定必要的行。 MVCC的实现方式有多种, 典型的有乐观(optimistic)并发控制 和 悲观(pessimistic)并发控制。 MVCC只在 READ COMMITTED(提交读) 和 REPEATABLE READ(可重读) 两个隔离级别下工作。其他两个隔离级别够和MVCC不兼容, 因为 READ UNCOMMITTED 总是读取最新的数据行, 而不是符合当前事务版本的数据行。而 SERIALIZABLE 则会对所有读取的行都加锁。(详情参见数据的事物隔离级别) 从书中可以了解到:MVCC是被Mysql中 事务型存储引擎InnoDB 所支持的;应对高并发事务, MVCC比单纯的加锁更高效; MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作; MVCC可以使用 乐观(optimistic)锁 和 悲观(pessimistic)锁来实现; 各数据库中MVCC实现并不统一,但是书中提到 “InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现的”(网上也有很多此类观点), 但其实并不准确, 可以参考 MySQL官方文档, 可以看到, InnoDB存储引擎在数据库每行数据的后面添加了三个字段, 不是两个!! 相关概念read view, 快照snapshot 淘宝数据库内核月报/2017/10/01/此文虽然是以PostgreSQL进行的说明, 但并不影响理解, 在”事务快照的实现”该部分有细节需要注意:事务快照是用来存储数据库的事务运行情况。一个事务快照的创建过程可以概括为：查看当前所有的未提交并活跃的事务，存储在数组中选取未提交并活跃的事务中最小的XID，记录在快照的xmin中选取所有已提交事务中最大的XID，加1后记录在xmax中 注意: 上文中在PostgreSQL中snapshot的概念, 对应MySQL中, 其实就是你在网上看到的read view,快照这些概念;比如何登成就有关于Read view的介绍;而 此文 却仍是使用快照来介绍; read view 主要是用来做可见性判断的, 比较普遍的解释便是”本事务不可见的当前其他活跃事务“, 但正是该解释, 可能会造成一节理解上的误区, 所以此处提供两个参考, 供给大家避开理解误区:read view中的高水位low_limit_id 可以参考github issues和 知乎问答其实上面第1点中加粗部分也是相关高水位的介绍( 注意进行了+1 ) 另外, 对于read view快照的生成时机, 也非常关键, 正是因为生成时机的不同, 造成了RC,RR两种隔离级别的不同可见性;在innodb中(默认repeatable read级别), 事务在begin/start transaction之后的第一条select读操作后, 会创建一个快照(read view), 将当前系统中活跃的其他事务记录记录起来;在innodb中(默认repeatable committed级别), 事务中每条select语句都会创建一个快照(read view);参考 使用REPEATABLE READ隔离级别，快照是基于执行第一个读操作的时间。 使用READ COMMITTED隔离级别，快照被重置为每个一致的读取操作的时间。 undolog Undo log是InnoDB MVCC事务特性的重要组成部分。当我们对记录做了变更操作时就会产生undo记录，Undo记录默认被记录到系统表空间(ibdata)中，但从5.6开始，也可以使用独立的Undo 表空间。 Undo记录中存储的是老版本数据，当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。当版本链很长时，通常可以认为这是个比较耗时的操作 大多数对数据的变更操作包括INSERT/DELETE/UPDATE，其中INSERT操作在事务提交前只对当前事务可见，因此产生的Undo日志可以在事务提交后直接删除（谁会对刚插入的数据有可见性需求呢！！），而对于UPDATE/DELETE则需要维护多版本信息，在InnoDB里，UPDATE和DELETE操作产生的Undo日志被归成一类，即update_undo。 另外, 在回滚段中的undo logs分为: insert undo log 和 update undo log insert undo log : 事务对insert新记录时产生的undolog, 只在事务回滚时需要, 并且在事务提交后就可以立即丢弃。 update undo log : 事务对记录进行delete和update操作时产生的undo log, 不仅在事务回滚时需要, 一致性读也需要，所以不能随便删除，只有当数据库所使用的快照中不涉及该日志记录，对应的回滚日志才会被purge线程删除。 如果需要设置独立表空间，需要在初始化数据库实例的时候，指定独立表空间的数量。UNDO内部由多个回滚段组成，即 Rollback segment，一共有128个，保存在ibdata系统表空间中，分别从resg slot0 - resg slot127，每一个resg slot，也就是每一个回滚段，内部由1024个undo segment 组成。回滚段（rollback segment）分配如下： slot 0 ，预留给系统表空间；slot 1- 32，预留给临时表空间，每次数据库重启的时候，都会重建临时表空间；slot33-127，如果有独立表空间，则预留给UNDO独立表空间；如果没有，则预留给系统表空间； 回滚段中除去32个提供给临时表事务使用，剩下的 128-32=96个回滚段，可执行 96*1024 个并发事务操作，每个事务占用一个 undo segment slot，注意，如果事务中有临时表事务，还会在临时表空间中的 undo segment slot 再占用一个 undo segment slot，即占用2个undo segment slot。如果错误日志中有：Cannot find a free slot for an undo log。则说明并发的事务太多了，需要考虑下是否要分流业务。回滚段（rollback segment ）采用 轮询调度的方式来分配使用，如果设置了独立表空间，那么就不会使用系统表空间回滚段中undo segment，而是使用独立表空间的，同时，如果回顾段正在 Truncate操作，则不分配。 InnoDB存储引擎在数据库每行数据的后面添加了三个字段 6字节的事务ID(DB_TRX_ID)字段: 用来标识最近一次对本行记录做修改(insert|update)的事务的标识符, 即最后一次修改(insert|update)本行记录的事务id。至于delete操作，在innodb看来也不过是一次update操作，更新行中的一个特殊位将行表示为deleted, 并非真正删除。 7字节的回滚指针(DB_ROLL_PTR)字段: 指写入回滚段(rollback segment)的 undo log record (撤销日志记录记录)。如果一行记录被更新, 则 undo log record 包含 ‘重建该行记录被更新之前内容’ 所必须的信息。 6字节的DB_ROW_ID字段: 包含一个随着新行插入而单调递增的行ID, 当由innodb自动产生聚集索引时，聚集索引会包括这个行ID的值，否则这个行ID不会出现在任何索引中。结合聚簇索引的相关知识点, 我的理解是, 如果我们的表中没有主键或合适的唯一索引, 也就是无法生成聚簇索引的时候, InnoDB会帮我们自动生成聚集索引, 但聚簇索引会使用DB_ROW_ID的值来作为主键; 如果我们有自己的主键或者合适的唯一索引, 那么聚簇索引中也就不会包含 DB_ROW_ID 了。 可见性比较算法（这里每个比较算法后面的描述是建立在rr级别下，rc级别也是使用该比较算法,此处未做描述）设要读取的行的最后提交事务id(即当前数据行的稳定事务id)为 trx_id_current当前新开事务id为 new_id当前新开事务创建的快照read view 中最早的事务id为up_limit_id, 最迟的事务id为low_limit_id(注意这个low_limit_id=未开启的事务id=当前最大事务id+1) 1.trx_id_current &lt; up_limit_id, 这种情况比较好理解, 表示, 新事务在读取该行记录时, 该行记录的稳定事务ID是小于, 系统当前所有活跃的事务, 所以当前行稳定数据对新事务可见, 跳到步骤5. 2.trx_id_current &gt;= trx_id_last, 这种情况也比较好理解, 表示, 该行记录的稳定事务id是在本次新事务创建之后才开启的, 但是却在本次新事务执行第二个select前就commit了，所以该行记录的当前值不可见, 跳到步骤4。 3.trx_id_current &lt;= trx_id_last, 表示: 该行记录所在事务在本次新事务创建的时候处于活动状态，从up_limit_id到low_limit_id进行遍历，如果trx_id_current等于他们之中的某个事务id的话，那么不可见, 调到步骤4,否则表示可见。 4.从该行记录的 DB_ROLL_PTR 指针所指向的回滚段中取出最新的undo-log的版本号, 将它赋值该 trx_id_current，然后跳到步骤1重新开始判断。 5.将该可见行的值返回。 案例分析 下面是一个非常简版的演示事务对某行记录的更新过程, 当然, InnoDB引擎在内部要做的工作非常多: 案例 下面是一套比较算法的应用过程, 比较长 算法 当前读和快照读 MySQL的InnoDB存储引擎默认事务隔离级别是RR(可重复读), 是通过 “行排他锁(写锁)+MVCC” 一起实现的, 不仅可以保证可重复读, 还可以部分防止幻读, 而非完全防止; 为什么是部分防止幻读, 而不是完全防止? 效果: 在如果事务B在事务A执行中, insert了一条数据并提交, 事务A再次查询, 虽然读取的是undo中的旧版本数据(防止了部分幻读), 但是事务A中执行update或者delete都是可以成功的!!因为在innodb中的操作可以分为当前读(current read)和快照读(snapshot read) 3.快照读(snapshot read)简单的select操作(当然不包括 select … lock in share mode, select … for update) 4.当前读(current read) 官网文档 Locking Reads select … lock in share modeselect … for updateinsertupdatedelete在RR级别下，快照读是通过MVVC(多版本控制)和undo log来实现的，当前读是通过加record lock(记录锁)和gap lock(间隙锁)来实现的。innodb在快照读的情况下并没有真正的避免幻读, 但是在当前读的情况下避免了不可重复读和幻读!!! 小结1.一般我们认为MVCC有下面几个特点： 每行数据都存在一个版本，每次数据更新时都更新该版本 修改时Copy出当前版本, 然后随意修改，各个事务之间无干扰 保存时比较版本号，如果成功(commit)，则覆盖原记录, 失败则放弃copy(rollback) 就是每行都有版本号，保存时根据版本号决定是否成功，听起来含有乐观锁的味道, 因为这看起来正是，在提交的时候才能知道到底能否提交成功 2.而InnoDB实现MVCC的方式是: 事务以排他锁的形式修改原始数据 把修改前的数据存放于undo log，通过回滚指针与主数据关联 修改成功（commit）啥都不做，失败则恢复undo log中的数据（rollback） 二者最本质的区别是: 当修改数据时是否要排他锁定，如果锁定了还算不算是MVCC？ 3.二者最本质的区别是: 当修改数据时是否要排他锁定，如果锁定了还算不算是MVCC？ Innodb的实现真算不上MVCC, 因为并没有实现核心的多版本共存, undo log 中的内容只是串行化的结果, 记录了多个事务的过程, 不属于多版本共存。但理想的MVCC是难以实现的, 当事务仅修改一行记录使用理想的MVCC模式是没有问题的, 可以通过比较版本号进行回滚, 但当事务影响到多行数据时, 理想的MVCC就无能为力了。 比如, 如果事务A执行理想的MVCC, 修改Row1成功, 而修改Row2失败, 此时需要回滚Row1, 但因为Row1没有被锁定, 其数据可能又被事务B所修改, 如果此时回滚Row1的内容，则会破坏事务B的修改结果，导致事务B违反ACID。 这也正是所谓的 第一类更新丢失 的情况。 也正是因为InnoDB使用的MVCC中结合了排他锁, 不是纯的MVCC, 所以第一类更新丢失是不会出现了, 一般说更新丢失都是指第二类丢失更新。 补充 第一类更新丢失，回滚覆盖：撤消一个事务时，在该事务内的写操作要回滚，把其它已提交的事务写入的数据覆盖了。 第二类更新丢失，提交覆盖：提交一个事务时，写操作依赖于事务内读到的数据，读发生在其他事务提交前，写发生在其他事务提交后，把其他已提交的事务写入的数据覆盖了。这是不可重复读的特例。 参考 segmentfault","categories":[{"name":"mysql","slug":"mysql","permalink":"https://www.sakuraus.cn/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://www.sakuraus.cn/tags/mysql/"},{"name":"Mvcc","slug":"Mvcc","permalink":"https://www.sakuraus.cn/tags/Mvcc/"}]},{"title":"Raft 算法译文","slug":"Raft","date":"2018-12-20T08:33:50.154Z","updated":"2018-12-20T08:33:50.154Z","comments":true,"path":"2018/12/20/Raft/","link":"","permalink":"https://www.sakuraus.cn/2018/12/20/Raft/","excerpt":"","text":"寻找一种易于理解的一致性算法（扩展版）摘要Raft 是一种为了管理复制日志的一致性算法。它提供了和 Paxos 算法相同的功能和性能，但是它的算法结构和 Paxos 不同，使得 Raft 算法更加容易理解并且更容易构建实际的系统。为了提升可理解性，Raft 将一致性算法分解成了几个关键模块，例如领导人选举、日志复制和安全性。同时它通过实施一个更强的一致性来减少需要考虑的状态的数量。从一个用户研究的结果可以证明，对于学生而言，Raft 算法比 Paxos 算法更加容易学习。Raft 算法还包括一个新的机制来允许集群成员的动态改变，它利用重叠的大多数来保证安全性。 1 介绍一致性算法允许一组机器像一个整体一样工作，即使其中一些机器出现故障也能够继续工作下去。正因为如此，一致性算法在构建可信赖的大规模软件系统中扮演着重要的角色。在过去的 10 年里，Paxos 算法统治着一致性算法这一领域：绝大多数的实现都是基于 Paxos 或者受其影响。同时 Paxos 也成为了教学领域里讲解一致性问题时的示例。 但是不幸的是，尽管有很多工作都在尝试降低它的复杂性，但是 Paxos 算法依然十分难以理解。并且，Paxos 自身的算法结构需要进行大幅的修改才能够应用到实际的系统中。这些都导致了工业界和学术界都对 Paxos 算法感到十分头疼。 和 Paxos 算法进行过努力之后，我们开始寻找一种新的一致性算法，可以为构建实际的系统和教学提供更好的基础。我们的做法是不寻常的，我们的首要目标是可理解性：我们是否可以在实际系统中定义一个一致性算法，并且能够比 Paxos 算法以一种更加容易的方式来学习。此外，我们希望该算法方便系统构建者的直觉的发展。不仅一个算法能够工作很重要，而且能够显而易见的知道为什么能工作也很重要。 Raft 一致性算法就是这些工作的结果。在设计 Raft 算法的时候，我们使用一些特别的技巧来提升它的可理解性，包括算法分解（Raft 主要被分成了领导人选举，日志复制和安全三个模块）和减少状态机的状态（相对于 Paxos，Raft 减少了非确定性和服务器互相处于非一致性的方式）。一份针对两所大学 43 个学生的研究表明 Raft 明显比 Paxos 算法更加容易理解。在这些学生同时学习了这两种算法之后，和 Paxos 比起来，其中 33 个学生能够回答有关于 Raft 的问题。 Raft 算法在许多方面和现有的一致性算法都很相似（主要是 Oki 和 Liskov 的 Viewstamped Replication），但是它也有一些独特的特性： 强领导者：和其他一致性算法相比，Raft 使用一种更强的领导能力形式。比如，日志条目只从领导者发送给其他的服务器。这种方式简化了对复制日志的管理并且使得 Raft 算法更加易于理解。 领导选举：Raft 算法使用一个随机计时器来选举领导者。这种方式只是在任何一致性算法都必须实现的心跳机制上增加了一点机制。在解决冲突的时候会更加简单快捷。 成员关系调整：Raft 使用一种共同一致的方法来处理集群成员变换的问题，在这种方法下，处于调整过程中的两种不同的配置集群中大多数机器会有重叠，这就使得集群在成员变换的时候依然可以继续工作。 我们相信，Raft 算法不论出于教学目的还是作为实践项目的基础都是要比 Paxos 或者其他一致性算法要优异的。它比其他算法更加简单，更加容易理解；它的算法描述足以实现一个现实的系统；它有好多开源的实现并且在很多公司里使用；它的安全性已经被证明；它的效率和其他算法比起来也不相上下。 接下来，这篇论文会介绍以下内容：复制状态机问题（第 2 节），讨论 Paxos 的优点和缺点（第 3 节），讨论我们为了理解能力而使用的方法（第 4 节），阐述 Raft 一致性算法（第 5-8 节），评价 Raft 算法（第 9 节），以及一些相关的工作（第 10 节）。 2 复制状态机一致性算法是从复制状态机的背景下提出的（参考英文原文引用37）。在这种方法中，一组服务器上的状态机产生相同状态的副本，并且在一些机器宕掉的情况下也可以继续运行。复制状态机在分布式系统中被用于解决很多容错的问题。例如，大规模的系统中通常都有一个集群领导者，像 GFS、HDFS 和 RAMCloud，典型应用就是一个独立的的复制状态机去管理领导选举和存储配置信息并且在领导人宕机的情况下也要存活下来。比如 Chubby 和 ZooKeeper。 图 1 图 1 ：复制状态机的结构。一致性算法管理着来自客户端指令的复制日志。状态机从日志中处理相同顺序的相同指令，所以产生的结果也是相同的。复制状态机通常都是基于复制日志实现的，如图 1。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。 保证复制日志相同就是一致性算法的工作了。在一台服务器上，一致性模块接收客户端发送来的指令然后增加到自己的日志中去。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，尽管有些服务器会宕机。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成一个高可靠的状态机。 实际系统中使用的一致性算法通常含有以下特性： 安全性保证（绝对不会返回一个错误的结果）：在非拜占庭错误情况下，包括网络延迟、分区、丢包、冗余和乱序等错误都可以保证正确。 可用性：集群中只要有大多数的机器可运行并且能够相互通信、和客户端通信，就可以保证可用。因此，一个典型的包含 5 个节点的集群可以容忍两个节点的失败。服务器被停止就认为是失败。他们当有稳定的存储的时候可以从状态中恢复回来并重新加入集群。 不依赖时序来保证一致性：物理时钟错误或者极端的消息延迟在可能只有在最坏情况下才会导致可用性问题。 通常情况下，一条指令可以尽可能快的在集群中大多数节点响应一轮远程过程调用时完成。小部分比较慢的节点不会影响系统整体的性能。 3 Paxos 算法的问题在过去的 10 年里，Leslie Lamport 的 Paxos 算法几乎已经成为一致性的代名词：Paxos 是在课程教学中最经常使用的算法，同时也是大多数一致性算法实现的起点。Paxos 首先定义了一个能够达成单一决策一致的协议，比如单条的复制日志项。我们把这一子集叫做单决策 Paxos。然后通过组合多个 Paxos 协议的实例来促进一系列决策的达成。Paxos 保证安全性和活性，同时也支持集群成员关系的变更。Paxos 的正确性已经被证明，在通常情况下也很高效。 不幸的是，Paxos 有两个明显的缺点。第一个缺点是 Paxos 算法特别的难以理解。完整的解释是出了名的不透明；通过极大的努力之后，也只有少数人成功理解了这个算法。因此，有了几次用更简单的术语来解释 Paxos 的尝试。尽管这些解释都只关注了单决策的子集问题，但依然很具有挑战性。在 2012 年 NSDI 的会议中的一次调查显示，很少有人对 Paxos 算法感到满意，甚至在经验老道的研究者中也是如此。我们自己也尝试去理解 Paxos；我们一直没能理解 Paxos 直到我们读了很多对 Paxos 的简化解释并且设计了我们自己的算法之后，这一过程花了近一年时间。 我们假设 Paxos 的不透明性来自它选择单决策问题作为它的基础。单决策 Paxos 是晦涩微妙的，它被划分成了两种没有简单直观解释和无法独立理解的情景。因此，这导致了很难建立起直观的感受为什么单决策 Paxos 算法能够工作。构成多决策 Paxos 增加了很多错综复杂的规则。我们相信，在多决策上达成一致性的问题（一份日志而不是单一的日志记录）能够被分解成其他的方式并且更加直接和明显。 Paxos算法的第二个问题就是它没有提供一个足够好的用来构建一个现实系统的基础。一个原因是还没有一种被广泛认同的多决策问题的算法。Lamport 的描述基本上都是关于单决策 Paxos 的；他简要描述了实施多决策 Paxos 的方法，但是缺乏很多细节。当然也有很多具体化 Paxos 的尝试，但是他们都互相不一样，和 Paxos 的概述也不同。例如 Chubby 这样的系统实现了一个类似于 Paxos 的算法，但是大多数的细节并没有被公开。 而且，Paxos 算法的结构也不是十分易于构建实践的系统；单决策分解也会产生其他的结果。例如，独立的选择一组日志条目然后合并成一个序列化的日志并没有带来太多的好处，仅仅增加了不少复杂性。围绕着日志来设计一个系统是更加简单高效的；新日志条目以严格限制的顺序增添到日志中去。另一个问题是，Paxos 使用了一种对等的点对点的方式作为它的核心（尽管它最终提议了一种弱领导人的方法来优化性能）。在只有一个决策会被制定的简化世界中是很有意义的，但是很少有现实的系统使用这种方式。如果有一系列的决策需要被制定，首先选择一个领导人，然后让他去协调所有的决议，会更加简单快速。 因此，实际的系统中很少有和 Paxos 相似的实践。每一种实现都是从 Paxos 开始研究，然后发现很多实现上的难题，再然后开发了一种和 Paxos 明显不一样的结构。这样是非常费时和容易出错的，并且理解 Paxos 的难度使得这个问题更加糟糕。Paxos 算法在理论上被证明是正确可行的，但是现实的系统和 Paxos 差别是如此的大，以至于这些证明没有什么太大的价值。下面来自 Chubby 实现非常典型： 在Paxos算法描述和实现现实系统中间有着巨大的鸿沟。最终的系统建立在一种没有经过证明的算法之上。由于以上问题，我们认为 Paxos 算法既没有提供一个良好的基础给实践的系统，也没有给教学很好的帮助。基于一致性问题在大规模软件系统中的重要性，我们决定看看我们是否可以设计一个拥有更好特性的替代 Paxos 的一致性算法。Raft算法就是这次实验的结果。 4 为了可理解性的设计设计 Raft 算法我们有几个初衷：它必须提供一个完整的实际的系统实现基础，这样才能大大减少开发者的工作；它必须在任何情况下都是安全的并且在大多数的情况下都是可用的；并且它的大部分操作必须是高效的。但是我们最重要也是最大的挑战是可理解性。它必须保证对于普遍的人群都可以十分容易的去理解。另外，它必须能够让人形成直观的认识，这样系统的构建者才能够在现实中进行必然的扩展。 在设计 Raft 算法的时候，有很多的点需要我们在各种备选方案中进行选择。在这种情况下，我们评估备选方案基于可理解性原则：解释各个备选方案有多大的难度（例如，Raft 的状态空间有多复杂，是否有微妙的暗示）？对于一个读者而言，完全理解这个方案和暗示是否容易？ 我们意识到对这种可理解性分析上具有高度的主观性；尽管如此，我们使用了两种通常适用的技术来解决这个问题。第一个技术就是众所周知的问题分解：只要有可能，我们就将问题分解成几个相对独立的，可被解决的、可解释的和可理解的子问题。例如，Raft 算法被我们分成领导人选举，日志复制，安全性和角色改变几个部分。 我们使用的第二个方法是通过减少状态的数量来简化需要考虑的状态空间，使得系统更加连贯并且在可能的时候消除不确定性。特别的，所有的日志是不允许有空洞的，并且 Raft 限制了日志之间变成不一致状态的可能。尽管在大多数情况下我们都试图去消除不确定性，但是也有一些情况下不确定性可以提升可理解性。尤其是，随机化方法增加了不确定性，但是他们有利于减少状态空间数量，通过处理所有可能选择时使用相似的方法。我们使用随机化去简化 Raft 中领导人选举算法。 5 Raft 一致性算法Raft 是一种用来管理章节 2 中描述的复制日志的算法。图 2 为了参考之用，总结这个算法的简略版本，图 3 列举了这个算法的一些关键特性。图中的这些元素会在剩下的章节逐一介绍。 Raft 通过选举一个高贵的领导人，然后给予他全部的管理复制日志的责任来实现一致性。领导人从客户端接收日志条目，把日志条目复制到其他服务器上，并且当保证安全性的时候告诉其他的服务器应用日志条目到他们的状态机中。拥有一个领导人大大简化了对复制日志的管理。例如，领导人可以决定新的日志条目需要放在日志中的什么位置而不需要和其他服务器商议，并且数据都从领导人流向其他服务器。一个领导人可以宕机，可以和其他服务器失去连接，这时一个新的领导人会被选举出来。 通过领导人的方式，Raft 将一致性问题分解成了三个相对独立的子问题，这些问题会在接下来的子章节中进行讨论： 领导选举：一个新的领导人需要被选举出来，当现存的领导人宕机的时候（章节 5.2） 日志复制：领导人必须从客户端接收日志然后复制到集群中的其他节点，并且强制要求其他节点的日志保持和自己相同。 安全性：在 Raft 中安全性的关键是在图 3 中展示的状态机安全：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一个不同的指令。章节 5.4 阐述了 Raft 算法是如何保证这个特性的；这个解决方案涉及到一个额外的选举机制（5.2 节）上的限制。 在展示一致性算法之后，这一章节会讨论可用性的一些问题和系统中的候选人角色的问题。 状态： 状态 所有服务器上持久存在的 currentTerm 服务器最后一次知道的任期号（初始化为 0，持续递增） votedFor 在当前获得选票的候选人的 Id log[] 日志条目集；每一个条目包含一个用户状态机执行的指令，和收到时的任期号 状态 所有服务器上经常变的 commitIndex 已知的最大的已经被提交的日志条目的索引值 lastApplied 最后被应用到状态机的日志条目索引值（初始化为 0，持续递增） 状态 在领导人里经常改变的 （选举后重新初始化） nextIndex[] 对于每一个服务器，需要发送给他的下一个日志条目的索引值（初始化为领导人最后索引值加一） matchIndex[] 对于每一个服务器，已经复制给他的日志的最高索引值 附加日志 RPC： 由领导人负责调用来复制日志指令；也会用作heartbeat 参数 解释 term 领导人的任期号 leaderId 领导人的 Id，以便于跟随者重定向请求 prevLogIndex 新的日志条目紧随之前的索引值 prevLogTerm prevLogIndex 条目的任期号 entries[] 准备存储的日志条目（表示心跳时为空；一次性发送多个是为了提高效率） leaderCommit 领导人已经提交的日志的索引值 返回值 解释 term 当前的任期号，用于领导人去更新自己 success 跟随者包含了匹配上 prevLogIndex 和 prevLogTerm 的日志时为真 接收者实现： 如果 term &lt; currentTerm 就返回 false （5.1 节） 如果日志在 prevLogIndex 位置处的日志条目的任期号和 prevLogTerm 不匹配，则返回 false （5.3 节） 如果已经存在的日志条目和新的产生冲突（索引值相同但是任期号不同），删除这一条和之后所有的 （5.3 节） 附加任何在已有的日志中不存在的条目 如果 leaderCommit &gt; commitIndex，令 commitIndex 等于 leaderCommit 和 新日志条目索引值中较小的一个 请求投票 RPC： 由候选人负责调用用来征集选票（5.2 节） 参数 解释 term 候选人的任期号 candidateId 请求选票的候选人的 Id lastLogIndex 候选人的最后日志条目的索引值 lastLogTerm 候选人最后日志条目的任期号 返回值 解释 term 当前任期号，以便于候选人去更新自己的任期号 voteGranted 候选人赢得了此张选票时为真 接收者实现： 如果term &lt; currentTerm返回 false （5.2 节） 如果 votedFor 为空或者就是 candidateId，并且候选人的日志至少和自己一样新，那么就投票给他（5.2 节，5.4 节） 所有服务器需遵守的规则： 所有服务器： 如果commitIndex &gt; lastApplied，那么就 lastApplied 加一，并把log[lastApplied]应用到状态机中（5.3 节） 如果接收到的 RPC 请求或响应中，任期号T &gt; currentTerm，那么就令 currentTerm 等于 T，并切换状态为跟随者（5.1 节） 跟随者（5.2 节）： 响应来自候选人和领导者的请求 如果在超过选举超时时间的情况之前都没有收到领导人的心跳，或者是候选人请求投票的，就自己变成候选人 候选人（5.2 节）： 在转变成候选人后就立即开始选举过程 自增当前的任期号（currentTerm） 给自己投票 重置选举超时计时器 发送请求投票的 RPC 给其他所有服务器 如果接收到大多数服务器的选票，那么就变成领导人 如果接收到来自新的领导人的附加日志 RPC，转变成跟随者 如果选举过程超时，再次发起一轮选举 领导人： 一旦成为领导人：发送空的附加日志 RPC（心跳）给其他所有的服务器；在一定的空余时间之后不停的重复发送，以阻止跟随者超时（5.2 节） 如果接收到来自客户端的请求：附加条目到本地日志中，在条目被应用到状态机后响应客户端（5.3 节） 如果对于一个跟随者，最后日志条目的索引值大于等于 nextIndex，那么：发送从 nextIndex 开始的所有日志条目： 如果成功：更新相应跟随者的 nextIndex 和 matchIndex 如果因为日志不一致而失败，减少 nextIndex 重试 如果存在一个满足N &gt; commitIndex的 N，并且大多数的matchIndex[i] ≥ N成立，并且log[N].term == currentTerm成立，那么令 commitIndex 等于这个 N （5.3 和 5.4 节） 图 2 图 2：一个关于 Raft 一致性算法的浓缩总结（不包括成员变换和日志压缩）。| 特性| 解释||—|—||选举安全特性| 对于一个给定的任期号，最多只会有一个领导人被选举出来（5.2 节）||领导人只附加原则| 领导人绝对不会删除或者覆盖自己的日志，只会增加（5.3 节）||日志匹配原则| 如果两个日志在相同的索引位置的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同（5.3 节）||领导人完全特性|如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中（5.4 节）||状态机安全特性| 如果一个领导人已经在给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会提交一个不同的日志（5.4.3 节）| 图 3 图 3：Raft 在任何时候都保证以上的各个特性。 5.1 Raft 基础一个 Raft 集群包含若干个服务器节点；通常是 5 个，这允许整个系统容忍 2 个节点的失效。在任何时刻，每一个服务器节点都处于这三个状态之一：领导人、跟随者或者候选人。在通常情况下，系统中只有一个领导人并且其他的节点全部都是跟随者。跟随者都是被动的：他们不会发送任何请求，只是简单的响应来自领导者或者候选人的请求。领导人处理所有的客户端请求（如果一个客户端和跟随者联系，那么跟随者会把请求重定向给领导人）。第三种状态，候选人，是用来在 5.2 节描述的选举新领导人时使用。图 4 展示了这些状态和他们之间的转换关系；这些转换关系会在接下来进行讨论。 图 4 图 4：服务器状态。跟随者只响应来自其他服务器的请求。如果跟随者接收不到消息，那么他就会变成候选人并发起一次选举。获得集群中大多数选票的候选人将成为领导者。在一个任期内，领导人一直都会是领导人直到自己宕机了。 图 5 图 5：时间被划分成一个个的任期，每个任期开始都是一次选举。在选举成功后，领导人会管理整个集群直到任期结束。有时候选举会失败，那么这个任期就会没有领导人而结束。任期之间的切换可以在不同的时间不同的服务器上观察到。Raft 把时间分割成任意长度的任期，如图 5。任期用连续的整数标记。每一段任期从一次选举开始，就像章节 5.2 描述的一样，一个或者多个候选人尝试成为领导者。如果一个候选人赢得选举，然后他就在接下来的任期内充当领导人的职责。在某些情况下，一次选举过程会造成选票的瓜分。在这种情况下，这一任期会以没有领导人结束；一个新的任期（和一次新的选举）会很快重新开始。Raft 保证了在一个给定的任期内，最多只有一个领导者。 不同的服务器节点可能多次观察到任期之间的转换，但在某些情况下，一个节点也可能观察不到任何一次选举或者整个任期全程。任期在 Raft 算法中充当逻辑时钟的作用，这会允许服务器节点查明一些过期的信息比如陈旧的领导者。每一个节点存储一个当前任期号，这一编号在整个时期内单调的增长。当服务器之间通信的时候会交换当前任期号；如果一个服务器的当前任期号比其他人小，那么他会更新自己的编号到较大的编号值。如果一个候选人或者领导者发现自己的任期号过期了，那么他会立即恢复成跟随者状态。如果一个节点接收到一个包含过期的任期号的请求，那么他会直接拒绝这个请求。 Raft 算法中服务器节点之间通信使用远程过程调用（RPCs），并且基本的一致性算法只需要两种类型的 RPCs。请求投票（RequestVote） RPCs 由候选人在选举期间发起（章节 5.2），然后附加条目（AppendEntries）RPCs 由领导人发起，用来复制日志和提供一种心跳机制（章节 5.3）。第 7 节为了在服务器之间传输快照增加了第三种 RPC。当服务器没有及时的收到 RPC 的响应时，会进行重试， 并且他们能够并行的发起 RPCs 来获得最佳的性能。 5.2 领导人选举Raft 使用一种心跳机制来触发领导人选举。当服务器程序启动时，他们都是跟随者身份。一个服务器节点继续保持着跟随者状态只要他从领导人或者候选者处接收到有效的 RPCs。领导者周期性的向所有跟随者发送心跳包（即不包含日志项内容的附加日志项 RPCs）来维持自己的权威。如果一个跟随者在一段时间里没有接收到任何消息，也就是选举超时，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者。 要开始一次选举过程，跟随者先要增加自己的当前任期号并且转换到候选人状态。然后他会并行的向集群中的其他服务器节点发送请求投票的 RPCs 来给自己投票。候选人会继续保持着当前状态直到以下三件事情之一发生：(a) 他自己赢得了这次的选举，(b) 其他的服务器成为领导者，(c) 一段时间之后没有任何一个获胜的人。这些结果会分别的在下面的段落里进行讨论。 当一个候选人从整个集群的大多数服务器节点获得了针对同一个任期号的选票，那么他就赢得了这次选举并成为领导人。每一个服务器最多会对一个任期号投出一张选票，按照先来先服务的原则（注意：5.4 节在投票上增加了一点额外的限制）。要求大多数选票的规则确保了最多只会有一个候选人赢得此次选举（图 3 中的选举安全性）。一旦候选人赢得选举，他就立即成为领导人。然后他会向其他的服务器发送心跳消息来建立自己的权威并且阻止新的领导人的产生。 在等待投票的时候，候选人可能会从其他的服务器接收到声明它是领导人的附加日志项 RPC。如果这个领导人的任期号（包含在此次的 RPC中）不小于候选人当前的任期号，那么候选人会承认领导人合法并回到跟随者状态。 如果此次 RPC 中的任期号比自己小，那么候选人就会拒绝这次的 RPC 并且继续保持候选人状态。 第三种可能的结果是候选人既没有赢得选举也没有输：如果有多个跟随者同时成为候选人，那么选票可能会被瓜分以至于没有候选人可以赢得大多数人的支持。当这种情况发生的时候，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，没有其他机制的话，选票可能会被无限的重复瓜分。 Raft 算法使用随机选举超时时间的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决。为了阻止选票起初就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择。这样可以把服务器都分散开以至于在大多数情况下只有一个服务器会选举超时；然后他赢得选举并在其他服务器超时之前发送心跳包。同样的机制被用在选票瓜分的情况下。每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果；这样减少了在新的选举中另外的选票瓜分的可能性。9.3 节展示了这种方案能够快速的选出一个领导人。 领导人选举这个例子，体现了可理解性原则是如何指导我们进行方案设计的。起初我们计划使用一种排名系统：每一个候选人都被赋予一个唯一的排名，供候选人之间竞争时进行选择。如果一个候选人发现另一个候选人拥有更高的排名，那么他就会回到跟随者状态，这样高排名的候选人能够更加容易的赢得下一次选举。但是我们发现这种方法在可用性方面会有一点问题（如果高排名的服务器宕机了，那么低排名的服务器可能会超时并再次进入候选人状态。而且如果这个行为发生得足够快，则可能会导致整个选举过程都被重置掉）。我们针对算法进行了多次调整，但是每次调整之后都会有新的问题。最终我们认为随机重试的方法是更加明显和易于理解的。 5.3 日志复制一旦一个领导人被选举出来，他就开始为客户端提供服务。客户端的每一个请求都包含一条被复制状态机执行的指令。领导人把这条指令作为一条新的日志条目附加到日志中去，然后并行的发起附加条目 RPCs 给其他的服务器，让他们复制这条日志条目。当这条日志条目被安全的复制（下面会介绍），领导人会应用这条日志条目到它的状态机中然后把执行的结果返回给客户端。如果跟随者崩溃或者运行缓慢，再或者网络丢包，领导人会不断的重复尝试附加日志条目 RPCs （尽管已经回复了客户端）直到所有的跟随者都最终存储了所有的日志条目。 图 6 图 6：日志由有序序号标记的条目组成。每个条目都包含创建时的任期号（图中框中的数字），和一个状态机需要执行的指令。一个条目当可以安全的被应用到状态机中去的时候，就认为是可以提交了。日志以图 6 展示的方式组织。每一个日志条目存储一条状态机指令和从领导人收到这条指令时的任期号。日志中的任期号用来检查是否出现不一致的情况，同时也用来保证图 3 中的某些性质。每一条日志条目同时也都有一个整数索引值来表明它在日志中的位置。 领导人来决定什么时候把日志条目应用到状态机中是安全的；这种日志条目被称为已提交。Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行。在领导人将创建的日志条目复制到大多数的服务器上的时候，日志条目就会被提交（例如在图 6 中的条目 7）。同时，领导人的日志中之前的所有日志条目也都会被提交，包括由其他领导人创建的条目。5.4 节会讨论某些当在领导人改变之后应用这条规则的隐晦内容，同时他也展示了这种提交的定义是安全的。领导人跟踪了最大的将会被提交的日志项的索引，并且索引值会被包含在未来的所有附加日志 RPCs （包括心跳包），这样其他的服务器才能最终知道领导人的提交位置。一旦跟随者知道一条日志条目已经被提交，那么他也会将这个日志条目应用到本地的状态机中（按照日志的顺序）。 我们设计了 Raft 的日志机制来维护一个不同服务器的日志之间的高层次的一致性。这么做不仅简化了系统的行为也使得更加可预计，同时他也是安全性保证的一个重要组件。Raft 维护着以下的特性，这些同时也组成了图 3 中的日志匹配特性： 如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们存储了相同的指令。 如果在不同的日志中的两个条目拥有相同的索引和任期号，那么他们之前的所有日志条目也全部相同。 第一个特性来自这样的一个事实，领导人最多在一个任期里在指定的一个日志索引位置创建一条日志条目，同时日志条目在日志中的位置也从来不会改变。第二个特性由附加日志 RPC 的一个简单的一致性检查所保证。在发送附加日志 RPC 的时候，领导人会把新的日志条目紧接着之前的条目的索引位置和任期号包含在里面。如果跟随者在它的日志中找不到包含相同索引位置和任期号的条目，那么他就会拒绝接收新的日志条目。一致性检查就像一个归纳步骤：一开始空的日志状态肯定是满足日志匹配特性的，然后一致性检查保护了日志匹配特性当日志扩展的时候。因此，每当附加日志 RPC 返回成功时，领导人就知道跟随者的日志一定是和自己相同的了。 在正常的操作中，领导人和跟随者的日志保持一致性，所以附加日志 RPC 的一致性检查从来不会失败。然而，领导人崩溃的情况会使得日志处于不一致的状态（老的领导人可能还没有完全复制所有的日志条目）。这种不一致问题会在领导人和跟随者的一系列崩溃下加剧。图 7 展示了跟随者的日志可能和新的领导人不同的方式。跟随者可能会丢失一些在新的领导人中有的日志条目，他也可能拥有一些领导人没有的日志条目，或者两者都发生。丢失或者多出日志条目可能会持续多个任期。 图 7 图 7：当一个领导人成功当选时，跟随者可能是任何情况（a-f）。每一个盒子表示是一个日志条目；里面的数字表示任期号。跟随者可能会缺少一些日志条目（a-b），可能会有一些未被提交的日志条目（c-d），或者两种情况都存在（e-f）。例如，场景 f 可能会这样发生，某服务器在任期 2 的时候是领导人，已附加了一些日志条目到自己的日志中，但在提交之前就崩溃了；很快这个机器就被重启了，在任期 3 重新被选为领导人，并且又增加了一些日志条目到自己的日志中；在任期 2 和任期 3 的日志被提交之前，这个服务器又宕机了，并且在接下来的几个任期里一直处于宕机状态。 在 Raft 算法中，领导人处理不一致是通过强制跟随者直接复制自己的日志来解决了。这意味着在跟随者中的冲突的日志条目会被领导人的日志覆盖。5.4 节会阐述如何通过增加一些限制来使得这样的操作是安全的。 要使得跟随者的日志进入和自己一致的状态，领导人必须找到最后两者达成一致的地方，然后删除从那个点之后的所有日志条目，发送自己的日志给跟随者。所有的这些操作都在进行附加日志 RPCs 的一致性检查时完成。领导人针对每一个跟随者维护了一个 nextIndex，这表示下一个需要发送给跟随者的日志条目的索引地址。当一个领导人刚获得权力的时候，他初始化所有的 nextIndex 值为自己的最后一条日志的index加1（图 7 中的 11）。如果一个跟随者的日志和领导人不一致，那么在下一次的附加日志 RPC 时的一致性检查就会失败。在被跟随者拒绝之后，领导人就会减小 nextIndex 值并进行重试。最终 nextIndex 会在某个位置使得领导人和跟随者的日志达成一致。当这种情况发生，附加日志 RPC 就会成功，这时就会把跟随者冲突的日志条目全部删除并且加上领导人的日志。一旦附加日志 RPC 成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持。 如果需要的话，算法可以通过减少被拒绝的附加日志 RPCs 的次数来优化。例如，当附加日志 RPC 的请求被拒绝的时候，跟随者可以包含冲突的条目的任期号和自己存储的那个任期的最早的索引地址。借助这些信息，领导人可以减小 nextIndex 越过所有那个任期冲突的所有日志条目；这样就变成每个任期需要一次附加条目 RPC 而不是每个条目一次。在实践中，我们十分怀疑这种优化是否是必要的，因为失败是很少发生的并且也不大可能会有这么多不一致的日志。 通过这种机制，领导人在获得权力的时候就不需要任何特殊的操作来恢复一致性。他只需要进行正常的操作，然后日志就能自动的在回复附加日志 RPC 的一致性检查失败的时候自动趋于一致。领导人从来不会覆盖或者删除自己的日志（图 3 的领导人只附加特性）。 日志复制机制展示出了第 2 节中形容的一致性特性：Raft 能够接受，复制并应用新的日志条目只要大部分的机器是工作的；在通常的情况下，新的日志条目可以在一次 RPC 中被复制给集群中的大多数机器；并且单个的缓慢的跟随者不会影响整体的性能。 5.4 安全性前面的章节里描述了 Raft 算法是如何选举和复制日志的。然而，到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令。例如，一个跟随者可能会进入不可用状态同时领导人已经提交了若干的日志条目，然后这个跟随者可能会被选举为领导人并且覆盖这些日志条目；因此，不同的状态机可能会执行不同的指令序列。 这一节通过在领导选举的时候增加一些限制来完善 Raft 算法。这一限制保证了任何的领导人对于给定的任期号，都拥有了之前任期的所有被提交的日志条目（图 3 中的领导人完整特性）。增加这一选举时的限制，我们对于提交时的规则也更加清晰。最终，我们将展示对于领导人完整特性的简要证明，并且说明领导人是如何领导复制状态机的做出正确行为的。 5.4.1 选举限制在任何基于领导人的一致性算法中，领导人都必须存储所有已经提交的日志条目。在某些一致性算法中，例如 Viewstamped Replication，某个节点即使是一开始并没有包含所有已经提交的日志条目，它也能被选为领导者。这些算法都包含一些额外的机制来识别丢失的日志条目并把他们传送给新的领导人，要么是在选举阶段要么在之后很快进行。不幸的是，这种方法会导致相当大的额外的机制和复杂性。Raft 使用了一种更加简单的方法，它可以保证所有之前的任期号中已经提交的日志条目在选举的时候都会出现在新的领导人中，不需要传送这些日志条目给领导人。这意味着日志条目的传送是单向的，只从领导人传给跟随者，并且领导人从不会覆盖自身本地日志中已经存在的条目。 Raft 使用投票的方式来阻止一个候选人赢得选举除非这个候选人包含了所有已经提交的日志条目。候选人为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的日志条目在这些服务器节点中肯定存在于至少一个节点上。如果候选人的日志至少和大多数的服务器节点一样新（这个新的定义会在下面讨论），那么他一定持有了所有已经提交的日志条目。请求投票 RPC 实现了这样的限制： RPC 中包含了候选人的日志信息，然后投票人会拒绝掉那些日志没有自己新的投票请求。 Raft 通过比较两份日志中最后一条日志条目的索引值和任期号定义谁的日志比较新。如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新。 5.4.2 提交之前任期内的日志条目如同 5.3 节介绍的那样，领导人知道一条当前任期内的日志记录是可以被提交的，只要它被存储到了大多数的服务器上。如果一个领导人在提交日志条目之前崩溃了，未来后续的领导人会继续尝试复制这条日志记录。然而，一个领导人不能断定一个之前任期里的日志条目被保存到大多数服务器上的时候就一定已经提交了。图 8 展示了一种情况，一条已经被存储到大多数节点上的老日志条目，也依然有可能会被未来的领导人覆盖掉。 图 8 图 8：如图的时间序列展示了为什么领导人无法决定对老任期号的日志条目进行提交。在 (a) 中，S1 是领导者，部分的复制了索引位置 2 的日志条目。在 (b) 中，S1 崩溃了，然后 S5 在任期 3 里通过 S3、S4 和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引 2 处。然后到 (c)，S5 又崩溃了；S1 重新启动，选举成功，开始复制日志。在这时，来自任期 2 的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。如果 S1 在 (d) 中又崩溃了，S5 可以重新被选举成功（通过来自 S2，S3 和 S4 的选票），然后覆盖了他们在索引 2 处的日志。反之，如果在崩溃之前，S1 把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如 (e) 中那样，那么在后面任期里面这些新的日志条目就会被提交（因为S5 就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目就会被提交。 为了消除图 8 里描述的情况，Raft 永远不会通过计算副本数目的方式去提交一个之前任期内的日志条目。只有领导人当前任期里的日志条目通过计算副本数目可以被提交；一旦当前任期的日志条目以这种方式被提交，那么由于日志匹配特性，之前的日志条目也都会被间接的提交。在某些情况下，领导人可以安全的知道一个老的日志条目是否已经被提交（例如，该条目是否存储到所有服务器上），但是 Raft 为了简化问题使用一种更加保守的方法。 当领导人复制之前任期里的日志时，Raft 会为所有日志保留原始的任期号, 这在提交规则上产生了额外的复杂性。在其他的一致性算法中，如果一个新的领导人要重新复制之前的任期里的日志时，它必须使用当前新的任期号。Raft 使用的方法更加容易辨别出日志，因为它可以随着时间和日志的变化对日志维护着同一个任期编号。另外，和其他的算法相比，Raft 中的新领导人只需要发送更少日志条目（其他算法中必须在他们被提交之前发送更多的冗余日志条目来为他们重新编号）。 5.4.3 安全性论证在给定了完整的 Raft 算法之后，我们现在可以更加精确的讨论领导人完整性特性（这一讨论基于 9.2 节的安全性证明）。我们假设领导人完全性特性是不存在的，然后我们推出矛盾来。假设任期 T 的领导人（领导人 T）在任期内提交了一条日志条目，但是这条日志条目没有被存储到未来某个任期的领导人的日志中。设大于 T 的最小任期 U 的领导人 U 没有这条日志条目。 图 9 图 9：如果 S1 （任期 T 的领导者）提交了一条新的日志在它的任期里，然后 S5 在之后的任期 U 里被选举为领导人，然后至少会有一个机器，如 S3，既拥有来自 S1 的日志，也给 S5 投票了。 在领导人 U 选举的时候一定没有那条被提交的日志条目（领导人从不会删除或者覆盖任何条目）。 领导人 T 复制这条日志条目给集群中的大多数节点，同时，领导人U 从集群中的大多数节点赢得了选票。因此，至少有一个节点（投票者、选民）同时接受了来自领导人T 的日志条目，并且给领导人U 投票了，如图 9。这个投票者是产生这个矛盾的关键。 这个投票者必须在给领导人 U 投票之前先接受了从领导人 T 发来的已经被提交的日志条目；否则他就会拒绝来自领导人 T 的附加日志请求（因为此时他的任期号会比 T 大）。 投票者在给领导人 U 投票时依然保有这条日志条目，因为任何中间的领导人都包含该日志条目（根据上述的假设），领导人从不会删除条目，并且跟随者只有和领导人冲突的时候才会删除条目。 投票者把自己选票投给领导人 U 时，领导人 U 的日志必须和投票者自己一样新。这就导致了两者矛盾之一。 首先，如果投票者和领导人 U 的最后一条日志的任期号相同，那么领导人 U 的日志至少和投票者一样长，所以领导人 U 的日志一定包含所有投票者的日志。这是另一处矛盾，因为投票者包含了那条已经被提交的日志条目，但是在上述的假设里，领导人 U 是不包含的。 除此之外，领导人 U 的最后一条日志的任期号就必须比投票人大了。此外，他也比 T 大，因为投票人的最后一条日志的任期号至少和 T 一样大（他包含了来自任期 T 的已提交的日志）。创建了领导人 U 最后一条日志的之前领导人一定已经包含了那条被提交的日志（根据上述假设，领导人 U 是第一个不包含该日志条目的领导人）。所以，根据日志匹配特性，领导人 U 一定也包含那条被提交当然日志，这里产生矛盾。 这里完成了矛盾。因此，所有比 T 大的领导人一定包含了所有来自 T 的已经被提交的日志。 日志匹配原则保证了未来的领导人也同时会包含被间接提交的条目，例如图 8 (d) 中的索引 2。 通过领导人完全特性，我们就能证明图 3 中的状态机安全特性，即如果已经服务器已经在某个给定的索引值应用了日志条目到自己的状态机里，那么其他的服务器不会应用一个不一样的日志到同一个索引值上。在一个服务器应用一条日志条目到他自己的状态机中时，他的日志必须和领导人的日志，在该条目和之前的条目上相同，并且已经被提交。现在我们来考虑在任何一个服务器应用一个指定索引位置的日志的最小任期；日志完全特性保证拥有更高任期号的领导人会存储相同的日志条目，所以之后的任期里应用某个索引位置的日志条目也会是相同的值。因此，状态机安全特性是成立的。 最后，Raft 要求服务器按照日志中索引位置顺序应用日志条目。和状态机安全特性结合起来看，这就意味着所有的服务器会应用相同的日志序列集到自己的状态机中，并且是按照相同的顺序。 5.5 跟随者和候选人崩溃到目前为止，我们都只关注了领导人崩溃的情况。跟随者和候选人崩溃后的处理方式比领导人要简单的多，并且他们的处理方式是相同的。如果跟随者或者候选人崩溃了，那么后续发送给他们的 RPCs 都会失败。Raft 中处理这种失败就是简单的通过无限的重试；如果崩溃的机器重启了，那么这些 RPC 就会完整的成功。如果一个服务器在完成了一个 RPC，但是还没有响应的时候崩溃了，那么在他重新启动之后就会再次收到同样的请求。Raft 的 RPCs 都是幂等的，所以这样重试不会造成任何问题。例如一个跟随者如果收到附加日志请求但是他已经包含了这一日志，那么他就会直接忽略这个新的请求。 5.6 时间和可用性Raft 的要求之一就是安全性不能依赖时间：整个系统不能因为某些事件运行的比预期快一点或者慢一点就产生了错误的结果。但是，可用性（系统可以及时的响应客户端）不可避免的要依赖于时间。例如，如果消息交换比服务器故障间隔时间长，候选人将没有足够长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作。 领导人选举是 Raft 中对时间要求最为关键的方面。Raft 可以选举并维持一个稳定的领导人,只要系统满足下面的时间要求： 广播时间（broadcastTime） &lt;&lt; 选举超时时间（electionTimeout） &lt;&lt; 平均故障间隔时间（MTBF）在这个不等式中，广播时间指的是从一个服务器并行的发送 RPCs 给集群中的其他服务器并接收响应的平均时间；选举超时时间就是在 5.2 节中介绍的选举的超时时间限制；然后平均故障间隔时间就是对于一台服务器而言，两次故障之间的平均时间。广播时间必须比选举超时时间小一个量级，这样领导人才能够发送稳定的心跳消息来阻止跟随者开始进入选举状态；通过随机化选举超时时间的方法，这个不等式也使得选票瓜分的情况变得不可能。选举超时时间应该要比平均故障间隔时间小上几个数量级，这样整个系统才能稳定的运行。当领导人崩溃后，整个系统会大约相当于选举超时的时间里不可用；我们希望这种情况在整个系统的运行中很少出现。 广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPCs 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 0.5 毫秒到 20 毫秒，取决于存储的技术。因此，选举超时时间可能需要在 10 毫秒到 500 毫秒之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的需求。 6 集群成员变化到目前为止，我们都假设集群的配置（加入到一致性算法的服务器集合）是固定不变的。但是在实践中，偶尔是会改变集群的配置的，例如替换那些宕机的机器或者改变复制级别。尽管可以通过暂停整个集群，更新所有配置，然后重启整个集群的方式来实现，但是在更改的时候集群会不可用。另外，如果存在手工操作步骤，那么就会有操作失误的风险。为了避免这样的问题，我们决定自动化配置改变并且将其纳入到 Raft 一致性算法中来。 为了让配置修改机制能够安全，那么在转换的过程中不能够存在任何时间点使得两个领导人同时被选举成功在同一个任期里。不幸的是，任何服务器直接从旧的配置直接转换到新的配置的方案都是不安全的。一次性自动的转换所有服务器是不可能的，所以在转换期间整个集群存在划分成两个独立的大多数群体的可能性（见图 10）。 图 10 图 10：直接从一种配置转到新的配置是十分不安全的，因为各个机器可能在任何的时候进行转换。在这个例子中，集群配额从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，两个不同的领导人在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置。为了保证安全性，配置更改必须使用两阶段方法。目前有很多种两阶段的实现。例如，有些系统在第一阶段停掉旧的配置所以集群就不能处理客户端请求；然后在第二阶段在启用新的配置。在 Raft 中，集群先切换到一个过渡的配置，我们称之为共同一致；一旦共同一致已经被提交了，那么系统就切换到新的配置上。共同一致是老配置和新配置的结合： 日志条目被复制给集群中新、老配置的所有服务器。 新、旧配置的服务器都可以成为领导人。 达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持。 共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程。此外，共同一致可以让集群在配置转换的过程人依然响应客户端的请求。 集群配置在复制日志中以特殊的日志条目来存储和通信；图 11 展示了配置转换的过程。当一个领导人接收到一个改变配置从 C-old 到 C-new 的请求，他会为了共同一致存储配置（图中的 C-old,new），以前面描述的日志条目和副本的形式。一旦一个服务器将新的配置日志条目增加到它的日志中，他就会用这个配置来做出未来所有的决定（服务器总是使用最新的配置，无论他是否已经被提交）。这意味着领导人要使用 C-old,new 的规则来决定日志条目 C-old,new 什么时候需要被提交。如果领导人崩溃了，被选出来的新领导人可能是使用 C-old 配置也可能是 C-old,new 配置，这取决于赢得选举的候选人是否已经接收到了 C-old,new 配置。在任何情况下， C-new 配置在这一时期都不会单方面的做出决定。 一旦 C-old,new 被提交，那么无论是 C-old 还是 C-new，在没有经过他人批准的情况下都不可能做出决定，并且领导人完全特性保证了只有拥有 C-old,new 日志条目的服务器才有可能被选举为领导人。这个时候，领导人创建一条关于 C-new 配置的日志条目并复制给集群就是安全的了。再者，每个服务器在见到新的配置的时候就会立即生效。当新的配置在 C-new 的规则下被提交，旧的配置就变得无关紧要，同时不使用新的配置的服务器就可以被关闭了。如图 11，C-old 和 C-new 没有任何机会同时做出单方面的决定；这保证了安全性。 图 11 图 11：一个配置切换的时间线。虚线表示已经被创建但是还没有被提交的条目，实线表示最后被提交的日志条目。领导人首先创建了 C-old,new 的配置条目在自己的日志中，并提交到 C-old,new 中（C-old 的大多数和 C-new 的大多数）。然后他创建 C-new 条目并提交到 C-new 中的大多数。这样就不存在 C-new 和 C-old 可以同时做出决定的时间点。在关于重新配置还有三个问题需要提出。第一个问题是，新的服务器可能初始化没有存储任何的日志条目。当这些服务器以这种状态加入到集群中，那么他们需要一段时间来更新追赶，这时还不能提交新的日志条目。为了避免这种可用性的间隔时间，Raft 在配置更新的时候使用了一种额外的阶段，在这个阶段，新的服务器以没有投票权身份加入到集群中来（领导人复制日志给他们，但是不考虑他们是大多数）。一旦新的服务器追赶上了集群中的其他机器，重新配置可以像上面描述的一样处理。 第二个问题是，集群的领导人可能不是新配置的一员。在这种情况下，领导人就会在提交了 C-new 日志之后退位（回到跟随者状态）。这意味着有这样的一段时间，领导人管理着集群，但是不包括他自己；他复制日志但是不把他自己算作是大多数之一。当 C-new 被提交时，会发生领导人过渡，因为这时是最早新的配置可以独立工作的时间点（将总是能够在 C-new 配置下选出新的领导人）。在此之前，可能只能从 C-old 中选出领导人。 第三个问题是，移除不在 C-new 中的服务器可能会扰乱集群。这些服务器将不会再接收到心跳，所以当选举超时，他们就会进行新的选举过程。他们会发送拥有新的任期号的请求投票 RPCs，这样会导致当前的领导人回退成跟随者状态。新的领导人最终会被选出来，但是被移除的服务器将会再次超时，然后这个过程会再次重复，导致整体可用性大幅降低。 为了避免这个问题，当服务器确认当前领导人存在时，服务器会忽略请求投票 RPCs。特别的，当服务器在当前最小选举超时时间内收到一个请求投票 RPC，他不会更新当前的任期号或者投出选票。这不会影响正常的选举，每个服务器在开始一次选举之前，至少等待一个最小选举超时时间。然而，这有利于避免被移除的服务器扰乱：如果领导人能够发送心跳给集群，那么他就不会被更大的任期号废黜。 7 日志压缩Raft 的日志在正常操作中不断的增长，但是在实际的系统中，日志不能无限制的增长。随着日志不断增长，他会占用越来越多的空间，花费越来越多的时间来重置。如果没有一定的机制去清除日志里积累的陈旧的信息，那么会带来可用性问题。 快照是最简单的压缩方法。在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃。快照技术被使用在 Chubby 和 ZooKeeper 中，接下来的章节会介绍 Raft 中的快照技术。 增量压缩的方法，例如日志清理或者日志结构合并树，都是可行的。这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力。首先，他们先选择一个已经积累的大量已经被删除或者被覆盖对象的区域，然后重写那个区域还活跃的对象，之后释放那个区域。和简单操作整个数据集合的快照相比，需要增加复杂的机制来实现。状态机可以实现 LSM tree 使用和快照相同的接口，但是日志清除方法就需要修改 Raft 了。 图 12 图 12：一个服务器用新的快照替换了从 1 到 5 的条目，快照值存储了当前的状态。快照中包含了最后的索引位置和任期号。图 12 展示了 Raft 中快照的基础思想。每个服务器独立的创建快照，只包括已经被提交的日志。主要的工作包括将状态机的状态写入到快照中。Raft 也包含一些少量的元数据到快照中：最后被包含索引指的是被快照取代的最后的条目在日志中的索引值（状态机最后应用的日志），最后被包含的任期指的是该条目的任期号。保留这些数据是为了支持快照后紧接着的第一个条目的附加日志请求时的一致性检查，因为这个条目需要最后的索引值和任期号。为了支持集群成员更新（第 6 节），快照中也将最后的一次配置作为最后一个条目存下来。一旦服务器完成一次快照，他就可以删除最后索引位置之前的所有日志和快照了。 尽管通常服务器都是独立的创建快照，但是领导人必须偶尔的发送快照给一些落后的跟随者。这通常发生在当领导人已经丢弃了下一条需要发送给跟随者的日志条目的时候。幸运的是这种情况不是常规操作：一个与领导人保持同步的跟随者通常都会有这个条目。然而一个运行非常缓慢的跟随者或者新加入集群的服务器（第 6 节）将不会有这个条目。这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给他们。 安装快照 RPC： 在领导人发送快照给跟随者时使用到。领导人总是按顺序发送。 参数 解释 term 领导人的任期号 leaderId 领导人的 Id，以便于跟随者重定向请求 lastIncludedIndex 快照中包含的最后日志条目的索引值 lastIncludedTerm 快照中包含的最后日志条目的任期号 offset 分块在快照中的偏移量 data[] 原始数据 done 如果这是最后一个分块则为 true 结果 解释 term 当前任期号，便于领导人更新自己 接收者实现： 如果term &lt; currentTerm就立即回复 如果是第一个分块（offset 为 0）就创建一个新的快照 在指定偏移量写入数据 如果 done 是 false，则继续等待更多的数据 保存快照文件，丢弃索引值小于快照的日志 如果现存的日志拥有相同的最后任期号和索引值，则后面的数据继续保持 丢弃整个日志 使用快照重置状态机 图 13 图 13：一个关于安装快照的简要概述。为了便于传输，快照都是被分成分块的；每个分块都给了跟随者生命的迹象，所以跟随者可以重置选举超时计时器。 在这种情况下领导人使用一种叫做安装快照的新的 RPC 来发送快照给太落后的跟随者；见图 13。当跟随者通过这种 RPC 接收到快照时，他必须自己决定对于已经存在的日志该如何处理。通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者直接丢弃他所有的日志；这些会被快照所取代，但是可能会和没有提交的日志产生冲突。如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照之后的条目必须正确和保留。 这种快照的方式背离了 Raft 的强领导人原则，因为跟随者可以在不知道领导人情况下创建快照。但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的。数据依然是从领导人传给跟随者，只是跟随者可以重新组织他们的数据了。 我们考虑过一种替代的基于领导人的快照方案，即只有领导人创建快照，然后发送给所有的跟随者。但是这样做有两个缺点。第一，发送快照会浪费网络带宽并且延缓了快照处理的时间。每个跟随者都已经拥有了所有产生快照需要的信息，而且很显然，自己从本地的状态中创建快照比通过网络接收别人发来的要经济。第二，领导人的实现会更加复杂。例如，领导人需要发送快照的同时并行的将新的日志条目发送给跟随者，这样才不会阻塞新的客户端请求。 还有两个问题影响了快照的性能。首先，服务器必须决定什么时候应该创建快照。如果快照创建的过于频繁，那么就会浪费大量的磁盘带宽和其他资源；如果创建快照频率太低，他就要承受耗尽存储容量的风险，同时也增加了从日志重建的时间。一个简单的策略就是当日志大小达到一个固定大小的时候就创建一次快照。如果这个阈值设置的显著大于期望的快照的大小，那么快照对磁盘压力的影响就会很小了。 第二个影响性能的问题就是写入快照需要花费显著的一段时间，并且我们还不希望影响到正常操作。解决方案是通过写时复制的技术，这样新的更新就可以被接收而不影响到快照。例如，具有函数式数据结构的状态机天然支持这样的功能。另外，操作系统的写时复制技术的支持（如 Linux 上的 fork）可以被用来创建完整的状态机的内存快照（我们的实现就是这样的）。 8 客户端交互这一节将介绍客户端是如何和 Raft 进行交互的，包括客户端如何发现领导人和 Raft 是如何支持线性化语义的。这些问题对于所有基于一致性的系统都存在，并且 Raft 的解决方案和其他的也差不多。 Raft 中的客户端发送所有请求给领导人。当客户端启动的时候，他会随机挑选一个服务器进行通信。如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供他最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）。如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程。 我们 Raft 的目标是要实现线性化语义（每一次操作立即执行，只执行一次，在他调用和收到回复之间）。但是，如上述，Raft 是可以执行同一条命令多次的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。 只读的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回脏数据的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是他还不知道。线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全特性保证了领导人一定拥有所有已经被提交的日志条目，但是在他任期开始的时候，他可能不知道那些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废黜了（他自己的信息已经变脏了如果一个更新的领导人被选举出来）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。可选的，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时间来保证安全性（假设时间误差是有界的）。 9 算法实现和评估我们已经为 RAMCloud 实现了 Raft 算法作为存储配置信息的复制状态机的一部分，并且帮助 RAMCloud 协调故障转移。这个 Raft 实现包含大约 2000 行 C++ 代码，其中不包括测试、注释和空行。这些代码是开源的。同时也有大约 25 个其他独立的第三方的基于这篇论文草稿的开源实现，针对不同的开发场景。同时，很多公司已经部署了基于 Raft 的系统。 这一节会从三个方面来评估 Raft 算法：可理解性、正确性和性能。 9.1 可理解性为了和 Paxos 比较 Raft 算法的可理解能力，我们针对高层次的本科生和研究生，在斯坦福大学的高级操作系统课程和加州大学伯克利分校的分布式计算课程上，进行了一次学习的实验。我们分别拍了针对 Raft 和 Paxos 的视频课程，并准备了相应的小测验。Raft 的视频讲课覆盖了这篇论文的所有内容除了日志压缩；Paxos 讲课包含了足够的资料来创建一个等价的复制状态机，包括单决策 Paxos，多决策 Paxos，重新配置和一些实际系统需要的性能优化（例如领导人选举）。小测验测试一些对算法的基本理解和解释一些边角的示例。每个学生都是看完第一个视频，回答相应的测试，再看第二个视频，回答相应的测试。大约有一半的学生先进行 Paxos 部分，然后另一半先进行 Raft 部分，这是为了说明两者独立的区别从第一个算法处学来的经验。我们计算参加人员的每一个小测验的得分来看参与者是否在 Raft 算法上更加容易理解。 我们尽可能的使得 Paxos 和 Raft 的比较更加公平。这个实验偏爱 Paxos 表现在两个方面：43 个参加者中有 15 个人在之前有一些 Paxos 的经验，并且 Paxos 的视频要长 14%。如表格 1 总结的那样，我们采取了一些措施来减轻这种潜在的偏见。我们所有的材料都可供审查。 关心 缓和偏见采取的手段 可供查看的材料 相同的讲课质量 两者使用同一个讲师。Paxos 使用的是现在很多大学里经常使用的。Paxos 会长 14%。 视频 相同的测验难度 问题以难度分组，在两个测验里成对出现。 小测验 公平评分 使用红字标题。随机顺序打分，两个测验交替进行。 红字标题 表 1：考虑到可能会存在的偏见，对于每种情况的解决方法，和相应的材料。参加者平均在 Raft 的测验中比 Paxos 高 4.9 分（总分 60，那么 Raft 的平均得分是 25.7，而 Paxos 是 20.8）；图 14 展示了每个参与者的得分。一对 t -测试表明，拥有 95% 的可信度，真实的 Raft 分数分布至少比 Paxos 高 2.5 分。 图 14 图 14：一个散点图表示了 43 个学生在 Paxos 和 Raft 的小测验中的成绩。在对角线之上的点表示在 Raft 获得了更高分数的学生。我们也建立了一个线性回归模型来预测一个新的学生的测验成绩，基于以下三个因素：他们使用的是哪个小测验，之前对 Paxos 的经验，和学习算法的顺序。模型显示，对小测验的选择会产生 12.5 分的差别在对 Raft 的好感度上。这显著的高于之前的 4.9 分，因为很多学生在之前都已经有了对于 Paxos 的经验，这相当明显的帮助 Paxos，对 Raft 就没什么太大影响了。但是奇怪的是，模型预测对于先进性 Paxos 小测验的人而言，Raft 的小测验得分会比 Paxos 低 6.3 分；我们不知道为什么，但这在统计学上是这样的。 我们同时也在测验之后调查了参与者，他们认为哪个算法更加容易实现和解释；这个的结果在图 15 上。压倒性的结果表明 Raft 算法更加容易实现和解释（41 人中的 33个）。但是，这种自己报告的结果不如参与者的成绩更加可信，并且参与者可能因为我们的 Raft 更加易于理解的假说而产生偏见。 图 15 图 15：通过一个 5 分制的问题，参与者（左边）被问哪个算法他们觉得在一个高效正确的系统里更容易实现，右边被问哪个更容易向学生解释。关于 Raft 用户学习有一个更加详细的讨论。 9.2 正确性在第 5 节，我们已经进行了一个正式的说明，和对一致性机制的安全性证明。这个正式说明让图 2 中的信息非常清晰通过 TLA+ 说明语言。大约 400 行说明充当了证明的主题。同时对于任何想实现的人也是十分有用的。我们非常机械的证明了日志完全特性通过 TLA 证明系统。然而，这个证明依赖的约束前提还没有被机械证明（例如，我们还没有证明这个说明中的类型安全）。而且，我们已经写了一个非正式的证明关于状态机安全性质是完备的，并且是相当清晰的（大约 3500 个词）。 9.3 性能Raft 和其他一致性算法例如 Paxos 有着差不多的性能。在性能方面，最重要的关注点是，当领导人被选举成功时，什么时候复制新的日志条目。Raft 通过很少数量的消息包（一轮从领导人到集群大多数机器的消息）就达成了这个目的。同时，进一步提升 Raft 的性能也是可行的。例如，很容易通过支持批量操作和管道操作来提高吞吐量和降低延迟。对于其他一致性算法已经提出过很多性能优化方案；其中有很多也可以应用到 Raft 中来，但是我们暂时把这个问题放到未来的工作中去。 我们使用我们自己的 Raft 实现来衡量 Raft 领导人选举的性能并且回答两个问题。首先，领导人选举的过程收敛是否快速？第二，在领导人宕机之后，最小的系统宕机时间是多久？ 图 16 图 16：发现并替换一个已经崩溃的领导人的时间。上面的图考察了在选举超时时间上的随机化程度，下面的图考察了最小超时时间。每条线代表了 1000 次实验（除了 150-150 毫秒只试了 100 次），和相应的确定的选举超时时间。例如，150-155 毫秒意思是，选举超时时间从这个区间范围内随机选择并确定下来。这个实验在一个拥有 5 个节点的集群上进行，其广播时延大约是 15 毫秒。对于 9 个节点的集群，结果也差不多。为了衡量领导人选举，我们反复的使一个拥有五个节点的服务器集群的领导人宕机，并计算需要多久才能发现领导人已经宕机并选出一个新的领导人（见图 16）。为了构建一个最坏的场景，在每一的尝试里，服务器都有不同长度的日志，意味着有些候选人是没有成为领导人的资格的。另外，为了促成选票瓜分的情况，我们的测试脚本在终止领导人之前同步的发送了一次心跳广播（这大约和领导人在崩溃前复制一个新的日志给其他机器很像）。领导人均匀的随机的在心跳间隔里宕机，也就是最小选举超时时间的一半。因此，最小宕机时间大约就是最小选举超时时间的一半。 图 16 上面的图表表明，只需要在选举超时时间上使用很少的随机化就可以大大避免选票被瓜分的情况。在没有随机化的情况下，在我们的测试里，选举过程往往都需要花费超过 10 秒钟由于太多的选票瓜分的情况。仅仅增加 5 毫秒的随机化时间，就大大的改善了选举过程，现在平均的宕机时间只有 287 毫秒。增加更多的随机化时间可以大大改善最坏情况：通过增加 50 毫秒的随机化时间，最坏的完成情况（1000 次尝试）只要 513 毫秒。 图 16 中下面的图显示，通过减少选举超时时间可以减少系统的宕机时间。在选举超时时间为 12-24 毫秒的情况下，只需要平均 35 毫秒就可以选举出新的领导人（最长的一次花费了 152 毫秒）。然而，进一步降低选举超时时间的话就会违反 Raft 的时间不等式需求：在选举新领导人之前，领导人就很难发送完心跳包。这会导致没有意义的领导人改变并降低了系统整体的可用性。我们建议使用更为保守的选举超时时间，比如 150-300 毫秒；这样的时间不大可能导致没有意义的领导人改变，而且依然提供不错的可用性。 10 相关工作已经有很多关于一致性算法的工作被发表出来，其中很多都可以归到下面的类别中： Lamport 关于 Paxos 的原始描述，和尝试描述的更清晰。 关于 Paxos 的更详尽的描述，补充遗漏的细节并修改算法，使得可以提供更加容易的实现基础。 实现一致性算法的系统，例如 Chubby，ZooKeeper 和 Spanner。对于 Chubby 和 Spanner 的算法并没有公开发表其技术细节，尽管他们都声称是基于 Paxos 的。ZooKeeper 的算法细节已经发表，但是和 Paxos 着实有着很大的差别。 Paxos 可以应用的性能优化。 Oki 和 Liskov 的 Viewstamped Replication（VR），一种和 Paxos 差不多的替代算法。原始的算法描述和分布式传输协议耦合在了一起，但是核心的一致性算法在最近的更新里被分离了出来。VR 使用了一种基于领导人的方法，和 Raft 有很多相似之处。 Raft 和 Paxos 最大的不同之处就在于 Raft 的强领导特性：Raft 使用领导人选举作为一致性协议里必不可少的部分，并且将尽可能多的功能集中到了领导人身上。这样就可以使得算法更加容易理解。例如，在 Paxos 中，领导人选举和基本的一致性协议是正交的：领导人选举仅仅是性能优化的手段，而且不是一致性所必须要求的。但是，这样就增加了多余的机制：Paxos 同时包含了针对基本一致性要求的两阶段提交协议和针对领导人选举的独立的机制。相比较而言，Raft 就直接将领导人选举纳入到一致性算法中，并作为两阶段一致性的第一步。这样就减少了很多机制。 像 Raft 一样，VR 和 ZooKeeper 也是基于领导人的，因此他们也拥有一些 Raft 的优点。但是，Raft 比 VR 和 ZooKeeper 拥有更少的机制因为 Raft 尽可能的减少了非领导人的功能。例如，Raft 中日志条目都遵循着从领导人发送给其他人这一个方向：附加条目 RPC 是向外发送的。在 VR 中，日志条目的流动是双向的（领导人可以在选举过程中接收日志）；这就导致了额外的机制和复杂性。根据 ZooKeeper 公开的资料看，它的日志条目也是双向传输的，但是它的实现更像 Raft。 和上述我们提及的其他基于一致性的日志复制算法中，Raft 的消息类型更少。例如，我们数了一下 VR 和 ZooKeeper 使用的用来基本一致性需要和成员改变的消息数（排除了日志压缩和客户端交互，因为这些都比较独立且和算法关系不大）。VR 和 ZooKeeper 都分别定义了 10 中不同的消息类型，相对的，Raft 只有 4 中消息类型（两种 RPC 请求和对应的响应）。Raft 的消息都稍微比其他算法的要信息量大，但是都很简单。另外，VR 和 ZooKeeper 都在领导人改变时传输了整个日志；所以为了能够实践中使用，额外的消息类型就很必要了。 Raft 的强领导人模型简化了整个算法，但是同时也排斥了一些性能优化的方法。例如，平等主义 Paxos （EPaxos）在某些没有领导人的情况下可以达到很高的性能。平等主义 Paxos 充分发挥了在状态机指令中的交换性。任何服务器都可以在一轮通信下就提交指令，除非其他指令同时被提出了。然而，如果指令都是并发的被提出，并且互相之间不通信沟通，那么 EPaxos 就需要额外的一轮通信。因为任何服务器都可以提交指令，所以 EPaxos 在服务器之间的负载均衡做的很好，并且很容易在 WAN 网络环境下获得很低的延迟。但是，他在 Paxos 上增加了非常明显的复杂性。 一些集群成员变换的方法已经被提出或者在其他的工作中被实现，包括 Lamport 的原始的讨论，VR 和 SMART。我们选择使用共同一致的方法因为他对一致性协议的其他部分影响很小，这样我们只需要很少的一些机制就可以实现成员变换。Lamport 的基于 α 的方法之所以没有被 Raft 选择是因为它假设在没有领导人的情况下也可以达到一致性。和 VR 和 SMART 相比较，Raft 的重新配置算法可以在不限制正常请求处理的情况下进行；相比较的，VR 需要停止所有的处理过程，SMART 引入了一个和 α 类似的方法，限制了请求处理的数量。Raft 的方法同时也需要更少的额外机制来实现，和 VR、SMART 比较而言。 11 结论算法的设计通常会把正确性，效率或者简洁作为主要的目标。尽管这些都是很有意义的目标，但是我们相信，可理解性也是一样的重要。在开发者把算法应用到实际的系统中之前，这些目标没有一个会被实现，这些都会必然的偏离发表时的形式。除非开发人员对这个算法有着很深的理解并且有着直观的感觉，否则将会对他们而言很难在实现的时候保持原有期望的特性。 在这篇论文中，我们尝试解决分布式一致性问题，但是一个广为接受但是十分令人费解的算法 Paxos 已经困扰了无数学生和开发者很多年了。我们创造了一种新的算法 Raft，显而易见的比 Paxos 要容易理解。我们同时也相信，Raft 也可以为实际的实现提供坚实的基础。把可理解性作为设计的目标改变了我们设计 Raft 的方式；这个过程是我们发现我们最终很少有技术上的重复，例如问题分解和简化状态空间。这些技术不仅提升了 Raft 的可理解性，同时也使我们坚信其正确性。 12 感谢这项研究必须感谢以下人员的支持：Ali Ghodsi，David Mazie`res，和伯克利 CS 294-91 课程、斯坦福 CS 240 课程的学生。Scott Klemmer 帮我们设计了用户调查，Nelson Ray 建议我们进行统计学的分析。在用户调查时使用的关于 Paxos 的幻灯片很大一部分是从 Lorenzo Alvisi 的幻灯片上借鉴过来的。特别的，非常感谢 DavidMazieres 和 Ezra Hoch，他们找到了 Raft 中一些难以发现的漏洞。许多人提供了关于这篇论文十分有用的反馈和用户调查材料，包括 Ed Bugnion，Michael Chan，Hugues Evrard，Daniel Giffin，Arjun Gopalan，Jon Howell，Vimalkumar Jeyakumar，Ankita Kejriwal，Aleksandar Kracun，Amit Levy，Joel Martin，Satoshi Matsushita，Oleg Pesok，David Ramos，Robbert van Renesse，Mendel Rosenblum，Nicolas Schiper，Deian Stefan，Andrew Stone，Ryan Stutsman，David Terei，Stephen Yang，Matei Zaharia 以及 24 位匿名的会议审查人员（可能有重复），并且特别感谢我们的领导人 Eddie Kohler。Werner Vogels 发了一条早期草稿链接的推特，给 Raft 带来了极大的关注。我们的工作由 Gigascale 系统研究中心和 Multiscale 系统研究中心给予支持，这两个研究中心由关注中心研究程序资金支持，一个是半导体研究公司的程序，由 STARnet 支持，一个半导体研究公司的程序由 MARCO 和 DARPA 支持，在国家科学基金会的 0963859 号批准，并且获得了来自 Facebook，Google，Mellanox，NEC，NetApp，SAP 和 Samsung 的支持。Diego Ongaro 由 Junglee 公司，斯坦福的毕业团体支持。 参考https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md","categories":[{"name":"raft","slug":"raft","permalink":"https://www.sakuraus.cn/categories/raft/"},{"name":"分布式","slug":"raft/分布式","permalink":"https://www.sakuraus.cn/categories/raft/分布式/"}],"tags":[{"name":"raft","slug":"raft","permalink":"https://www.sakuraus.cn/tags/raft/"},{"name":"分布式","slug":"分布式","permalink":"https://www.sakuraus.cn/tags/分布式/"}]},{"title":"docker-swarm","slug":"docker-swarm","date":"2018-11-26T13:16:31.666Z","updated":"2018-11-26T13:16:31.666Z","comments":true,"path":"2018/11/26/docker-swarm/","link":"","permalink":"https://www.sakuraus.cn/2018/11/26/docker-swarm/","excerpt":"","text":"#Docker swarm ##介绍，为什么需要Docker swarm ##Docker swarm 跟docker的关系 ##创建dockr swarm集群实现伸缩调度 一、Docker swarm 介绍 Swarm是Docker公司推出的用来管理docker集群，它将一群Docker宿主机变成一个单一的，虚拟的主机。Swarm使用标准的Docker API接口作为其前端访问入口，换言之，各种形式的Docker Client(docker client in Go, docker_py, docker等)均可以直接与Swarm通信。 Swarm几乎全部用go语言来完成开发，Swarm0.2发布，相比0.1版本，0.2版本增加了一个新的策略来调度集群中的容器，使得在可用的节点上传播它们，以及支持更多的Docker命令以及集群驱动。 Swarm deamon只是一个调度器（Scheduler）加路由器(router)，Swarm自己不运行容器，它只是接受docker客户端发送过来的请求，调度适合的节点来运行容器，这意味着，即使Swarm由于某些原因挂掉了，集群中的节点也会照常运行，当Swarm重新恢复运行之后，它会收集重建集群信息． 结构图： docker_swarm.png 一、为什么要使用它？ 1、应用想要扩容到两台以上的服务器上，多台服务器总是比单台服务器复杂，可以使用docker-swarm进行集群化的管理跟伸缩 2、应用是否有高可用的要求，在docker swarm集群中有两种不同类型的节点，Master节点和Worker节点,其中的一个Master节点是Leader,如果当前Leader宕机不可用，其他健康的Master中的一台会自动成为Leader 。如果Worker节点宕机不可用，宕机节点上的容器实例会被重新调度到其他健康的Worker节点上。 ####一、关键概念 Swarm 集群的管理和编排是使用嵌入到docker引擎的SwarmKit，可以在docker初始化时启动swarm模式或者加入已存在的swarm Node 运行 Docker 的主机可以主动初始化一个 Swarm 集群或者加入一个已存在的 Swarm 集群，这样这个运行 Docker 的主机就成为一个 Swarm 集群的节点 ( node ) 节点分为管理 ( manager ) 节点和工作 ( worker ) 节点。 管理节点用于 Swarm 集群的管理， docker swarm 命令基本只能在管理节点执行（节点退出集群命令 docker swarm leave 可以在工作节点执行）。 一个 Swarm 集群可以有多个管理节点，但只有一个管理节点可以成为 leader ， leader 通过 raft 协议实现 工作节点是任务执行节点，管理节点将服务 ( service ) 下发至工作节点执行。管理节点默认也作为工作节点。你也可以通过配置让服务只运行在管理节点。 docker_swarm_node 服务和任务 任务 （ Task ）是 Swarm 中的最小的调度单位，目前来说就是一个单一的容器。 服务 （ Services ） 是指一组任务的集合，服务定义了任务的属性。 docker_swarm_relation ###docker swarm init 命令参考 docker_swarm_init –cert-expiry设置节点证书有效期 –dispatcher-heartbeat设置节点报告它们的健康状态间隔的时间。 –external-ca value设置集群使用一个外部CA来签发节点证书。value的格式为protocol=X,url=Y。protocol指定的是发送签名请求到外部CA所使用的协议。目前只支持cfssl。URL指定的是签名请求应该提交到哪个endpoint。 –force-new-cluster强制一个失去仲裁能力的集群的其中一个节点重启成为一单节点集群，而不丢失数据。 –listen-addr value在这个地址监听集群管理相关流量。默认是监听0.0.0.0:2377。也可以指定一个网络接口来监听这个接口的地址。例如–listen-addr eth0:2377。端口是可选的。如果仅指定IP地址或接口名称，端口就使用默认的2377。 –advertise-addr value指定通告给集群的节点的地址，这个地址用来给其它节点访问API和overlay网络通信。如果没有指定地址，docker将检查系统是否只有一个IP地址，如果是将使用这个地址并使用监听的端口(查看–listen-addr)。如果系统有多个IP地址，–advertise-addr就必须指定一个以便内部管理节点能够正常通信和overlay网络通信。也可以指定一个网络接口来通告接口的地址，例如–advertise-addr eth0:2377。端口是可选的。如果仅指定一个IP地址或接口名称，就使用端口2377。 –task-history-limit设置任务历史记录保留限制。1、初始化1docker swarm init --advertise-addr xx.xx.xx 2、加入集群12 docker swarm join-token worker #可以查看加入节点的tokendocker swarm join --token SWMTKN-1-1oxfayeqathm39flfmtuglt3l3xpdkemellw8iyom0h99h5ebu-e4tfrqla6uqgzjgo1r4t84rtt 47.98.109.204:2377 3、查看节点1docker node ls 4、部署服务在manager节点部署nginx服务，服务数量为10个，公开指定端口是8080映射容器80,使用nginx镜像1docker service create --replicas 3 -p 80:80 --name nginx nginx","categories":[{"name":"docker","slug":"docker","permalink":"https://www.sakuraus.cn/categories/docker/"},{"name":"docker-swarm","slug":"docker/docker-swarm","permalink":"https://www.sakuraus.cn/categories/docker/docker-swarm/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.sakuraus.cn/tags/docker/"},{"name":"docker-swarm","slug":"docker-swarm","permalink":"https://www.sakuraus.cn/tags/docker-swarm/"}]},{"title":"如何搭建一个微服务架构","slug":"docker-compose","date":"2018-11-26T13:16:31.665Z","updated":"2018-11-26T13:16:31.665Z","comments":true,"path":"2018/11/26/docker-compose/","link":"","permalink":"https://www.sakuraus.cn/2018/11/26/docker-compose/","excerpt":"","text":"docker-compose术语 服务 ( service )：一个应用容器，实际上可以运行多个相同镜像的实例。 项目 ( project )：由一组关联的应用容器组成的一个完整业务单元。 可见，一个项目可以由多个服务（容器）关联而成， Compose 面向项目进行管理。 Dockerfile dockerfile是构建镜像的一个语法文件 类似shell 脚本 的语法 详情参考docker docker-compose 是可以构建多个服务的编排文件 现在有几个版本的编排语法 基本都是兼容的主要高版本加了一些新语法 使用高版本的时候注意docker版本 下面我列举出构建一个示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128version: &quot;2&quot; #不同版本支持的语法不同具services: #服务 编排的好处在于不用手动的去一次运行镜像 实现自动化docker很轻松就能部署一个项目 swoft1: # 服务名称 container_name: swoft-server1 # 运行的容器名称 image: swofts # 指定为镜像名称或镜像 ID。如果镜像在本地不存在， Compose 将会尝试拉取这个镜像。 ports: # 暴露端口信息 前面对应宿主机的端口 : 后面对应容器里面开放的端口 - &quot;8003:9502&quot; - &quot;8004:9504&quot; - &quot;8005:8099&quot; links: # 容器依靠的其他服务 这种方式是通过查询同一个网络中的服务 - db - redis - consul-client volumes: # 共享卷 前面对应宿主机的地址:后面把宿主机的地址映射到容器里面的地址 - /www/swoft:/var/www/swoft stdin_open: true # network_mode: &quot;mynetwork&quot; #设置网络模式。使用和 docker run 的 --network 参数一样的值。 tty: true # 模拟一个伪终端 command: [/bin/bash] # 覆盖容器启动后默认执行的命令。 swoft2: container_name: swoft-server2 image: swofts ports: - &quot;8006:9502&quot; - &quot;8007:9504&quot; - &quot;8008:8099&quot; links: - db - redis - consul-client volumes: - /www/swoft:/var/www/swoft stdin_open: true network_mode: &quot;mynetwork&quot; tty: true command: [/bin/bash] redis: container_name: redis image: redis ports: - &quot;6378:6379&quot; volumes: - /etc/redis.conf:/usr/local/etc/redis/redis.conf network_mode: &quot;mynetwork&quot; db: container_name: mysql image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: 123456 ports: - &quot;3307:3306&quot; network_mode: &quot;mynetwork&quot; consul-server: container_name: consul-server image: consul ports: - &quot;32240:8300&quot; - &quot;32241:8301&quot; - &quot;32242:8301/udp&quot; - &quot;32243:8302/udp&quot; - &quot;32244:8302&quot; - &quot;32245:8500&quot; - &quot;32246:8600&quot; - &quot;32247:8600/udp&quot; stdin_open: true network_mode: &quot;mynetwork&quot; tty: true command: /bin/bash consul-server1: container_name: consul-server1 image: consul ports: - &quot;32252:8300&quot; - &quot;32253:8301&quot; - &quot;32254:8301/udp&quot; - &quot;32255:8302/udp&quot; - &quot;32256:8302&quot; - &quot;32257:8500&quot; - &quot;32258:8600&quot; - &quot;32259:8600/udp&quot; links: - consul-server stdin_open: true network_mode: &quot;mynetwork&quot; tty: true command: /bin/bash consul-server2: container_name: consul-server2 image: consul ports: - &quot;32260:8300&quot; - &quot;32261:8301&quot; - &quot;32262:8301/udp&quot; - &quot;32263:8302/udp&quot; - &quot;32264:8302&quot; - &quot;32265:8500&quot; - &quot;32266:8600&quot; - &quot;32267:8600/udp&quot; links: - consul-server stdin_open: true network_mode: &quot;mynetwork&quot; tty: true command: /bin/bash consul-client: container_name: consul-client image: consul ports: - &quot;32270:8300&quot; - &quot;32271:8301&quot; - &quot;32272:8301/udp&quot; - &quot;32273:8302/udp&quot; - &quot;32274:8302&quot; - &quot;32275:8500&quot; - &quot;32276:8600&quot; - &quot;32277:8600/udp&quot; links: - consul-server - consul-server1 - consul-server2 stdin_open: true network_mode: &quot;mynetwork&quot; tty: true command: /bin/bash PHP环境搭建、win7系统 一、docker软件的下载1、下载地址:https://www.docker.com/docker-windows2、安装docker文件，直接点击下一步操作，完成后会自动安装VM和Git这两个文件。 ①安装过程中，会出现找不到，boot2docker这个文件，这个文件是docker的依赖，需要下载拷贝到docker的指定文件目录，(在报错位置)。 ②安装过程中出现enter press contiune … 情况，需要重启系统，按F2进入配置IO环境。更改配置。3、安装完成后，docker会自动分配一个IP地址，默认的帐号和密码，使用Xshell进行连接操作。(帐号:docker，密码:tcuser)4、以上连接完成后，进行镜像拉取，使用命令:docker pull 镜像名称，把镜像进行本地映射。5、运行镜像，使用命令:docker run -i -t 镜像名称 /bin/bash 如果出错，按出错提示进行操作。6、docker run -d -v /data:/data -p 80:80 -p 1229:1229 registry.aliyuncs.com/lingdianit/dev:v3 /etc/rc.local //运行映射文件及端口，这里的配置，必须要和虚拟机中的映射文件一致,data/7、常用命令的使用。 docker -version //查看版本 docker pull/push/search 镜像名称 //下载/上传/搜索镜像文件 docker images //列出所有安装过的镜像。 docker start/stop/run/resart/kill 进程id //docker启动/停止/运行/重启/杀掉 如:docker start a62 docker ps //查看映射文件 二、Xshell软件的下载 Xshell主要使用的是连接docker。方便命令操作。 三、VM虚拟机的环境映射1、本地磁盘中建立一个文件夹，后面需要映射的文件目录。2、启动docker时就已经启动VM虚拟机，在启动过程中有如下设置： 设置-&gt;共享文件价-&gt;添加共享文件-&gt;文件路径和文件目录(路径是本地文件的目录，文件目录是需要映射到虚拟机中的目录)。设置完成后需要重启虚拟机后生效。3、重启Xshell，查看文件夹中映射的文件是否存在，存在则配置成功。未配置成功查看错误日志。 四、域名的绑定设置 域名配置。C:\\Windows\\System32\\drivers\\etc\\hosts.txt,如下配置: 192.168.99.100 www.keloop.cn 192.168.99.100 staitc.keloop.cn 说明：前面是docker分配的IP地址，后面是配置访问的域名。 五、Git版本控制代码命令 1、使用SSH模式进行操作 2、全局配置： git config –global user.name “用户名” git config –global user.email 邮箱地址 3、常见命令 git init . //初始化 git add newfile //提交新文件到暂存区 git commit -m ‘add new file’ //提交到本地仓库 git remote add origin git@xx.com:demo.git //添加远程仓库地址 git fetch origin -p // 同步本地远程仓库镜像 git push origin dev:dev //把本地dev 推送到远端 git br dev //创建本地dev分支 git br feature-new origin/dev // 基于远端dev 创建新分支 feature-new git checkout -b feature-coupon origin/dev //创建分支 switched to a new branch “feature-coupon” //切换分支 git merge origin/dev //合并远端dev分支 密钥生成及配置(仓库和本地密钥必须一致) ssh-keygen -t rsa //生成密钥，产生id_rsa和id_rsa.pub 本地密钥放在:C:\\Users\\Administrator\\.ssh\\ gitlab设置密钥：设置-&gt;SSH密钥-&gt;添加密钥","categories":[{"name":"docker","slug":"docker","permalink":"https://www.sakuraus.cn/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.sakuraus.cn/tags/docker/"},{"name":"docker-compose","slug":"docker-compose","permalink":"https://www.sakuraus.cn/tags/docker-compose/"},{"name":"MicroService","slug":"MicroService","permalink":"https://www.sakuraus.cn/tags/MicroService/"}]},{"title":"docker-machine","slug":"docker-machine","date":"2018-11-26T13:16:31.665Z","updated":"2018-11-26T13:16:31.665Z","comments":true,"path":"2018/11/26/docker-machine/","link":"","permalink":"https://www.sakuraus.cn/2018/11/26/docker-machine/","excerpt":"","text":"docker-machine Docker Machine 介绍为什么需要Docker Machine Docker Machine 介绍 Docker Machine 是 Docker 官方编排项目之一，负责在多种平台上快速安装 Docker 环境。 Docker Machine 是一个工具，它允许你在虚拟宿主机上安装 Docker Engine ，并使用 docker-machine 命令管理这些宿主机。你可以使用 Machine 在你本地的 Mac 或 Windows box、公司网络、数据中心、或像 阿里云 或 华为云这样的云提供商上创建 Docker 宿主机。 使用 docker-machine 命令，你可以启动、审查、停止和重新启动托管的宿主机、升级 Docker 客户端和守护程序、并配置 Docker 客户端与你的宿主机通信 为什么要使用它？ 在没有Docker Machine之前，你可能会遇到以下问题： 1、你需要登录主机，按照主机及操作系统特有的安装以及配置步骤安装Docker，使其能运行Docker容器。 2、你需要研发一套工具管理多个Docker主机并监控其状态。Docker Machine的出现解决了以上问题。 1、Docker Machine简化了部署的复杂度，无论是在本机的虚拟机上还是在公有云平台，只需要一条命令便可搭建好Docker主机 2、Docker Machine提供了多平台多Docker主机的集中管理部署 3、Docker Machine 使应用由本地迁移到云端变得简单，只需要修改一下环境变量即可和任意Docker主机通信部署应用。 Docker的组成： 1、Docker daemon 2、一套与 Docker daemon 交互的 REST API 3、一个命令行客户端 docker-machine 安装1curl -L https://github.com/docker/machine/releases/download/v0.14.0/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp; \\&gt; install /tmp/docker-machine /usr/local/bin/docker-machine 使用 用Docker Machine可以批量安装和配置docker host，其支持在不同的环境下安装配置docker host，包括： 常规 Linux 操作系统 虚拟化平台 - VirtualBox、VMWare、Hyper-V 公有云 - Amazon Web Services、Microsoft Azure、Google Compute Engine、阿里、华为等 普通方式，仅供参考 创建一个名为cluster-master1 的主机，驱动方式是virtualbox1docker-machine create --driver virtualbox cluster-master1 报错提示没有发现VBoxManage。因此，需要手工安装 docker-machine 编辑yum 源1vim /etc/yum.repos.d/virtualbox.repo 写入以下信息1234567891011[virtualbox]name=Oracle Linux / RHEL / CentOS-$releasever / $basearch - VirtualBoxbaseurl=http://download.virtualbox.org/virtualbox/rpm/el/$releasever/$basearchenabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://www.virtualbox.org/download/oracle_vbox.ascyum search VirtualBox #查找具体安装版本yum install VirtualBox 还有可能出现报错内核不一致 根据提示下载指定的版本123yum install kernel-devel-3.10.0-862.2.3.el7.x86_64 yum install VirtualBoxvboxconfig #重新加载执行下，再次创建 继续报错，没有开启虚拟化，云服务器默认是不能开启的，云服务器有云服务器的驱动，目前阿里云、华为云有这种驱动，同时比如阿里云的驱动是可以在腾讯云使用，也可以在本地使用，这个不造成影响。 docker-machine 第三方驱动支持列表 https://github.com/docker/docker.github.io/blob/master/machine/AVAILABLE_DRIVER_PLUGINS.md 第三方驱动使用 1、下载驱动 二进制文件也可用，可以从以下链接下载： Mac OSX 64位： https://docker-machine-drivers.oss-cn-beijing.aliyuncs.com/docker-machine-driver-aliyunecs_darwin-amd64.tgz Linux 64位 : https://docker-machine-drivers.oss-cn-beijing.aliyuncs.com/docker-machine-driver-aliyunecs_linux-amd64.tgz Windows 64位： https://docker-machine-drivers.oss-cn-beijing.aliyuncs.com/docker-machine-driver-aliyunecs_windows-amd64.tgz 2、解压安装1234curl -L https://docker-machine-drivers.oss-cn-beijing.aliyuncs.com/docker-machine-driver-aliyunecs_linux-amd64.tgz tar xzvf driver-aliyunecs.tgz -C docker-machinemv ./bin/docker-machine-driver-aliyunecs.linux-amd64 /usr/local/bin/docker-machine-driver-aliyunecs chmod +x /usr/local/bin/docker-machine-driver-aliyunecs 想要创建一个阿里云虚拟化实例，需要满足几个条件 1、账户余额大于100，因为创建的实例为按量付费 2、设置accesskey，要具备操作账户的权限 阿里云驱动安装 登录阿里云账号控制台https://home.console.aliyun.com/new#/，选择accesskey 12345678docker-machine create -d aliyunecs --aliyunecs-io-optimized=optimized --aliyunecs-description=aliyunecs-machine-driver --aliyunecs-instance-type=ecs.mn4.small --aliyunecs-access-key-id=LTAIJIGa4sFefl1g --aliyunecs-access-key-secret=AlA7CV6zjntg7Q1zO3sGvIMIAxJi3m --aliyunecs-region=cn-hangzhou --aliyunecs-ssh-password=zaq1@wsxmanager –aliyunecs-io-optimized=optimized //磁盘io优化 –aliyunecs-description=aliyunecs-machine-driver //描述 –aliyunecs-instance-type=ecs.mn4.small //实例规格 –aliyunecs-access-key-id=LTxxxcxx // key –aliyunecs-access-key-secret=Axxx //秘钥 –aliyunecs-region=cn-hangzhou //地区 --aliyunecs-ssh-password=zaq1@wsx //ssh登录密码 –aliyunecs-image-id=centos_7_04_64_20G_alibase_201701015.vhd //镜像实例 docker-machine 管理 两台服务器 本地主机：47.98.147.4xx 远程主机：xxx.xxx.xxx Scp操作1docker-machine scp worker:/root/foo.txt . mount 操作云服务器，不能使用（可能） 手册 https://docs.docker.com/machine/install-machine/ 创建dockr虚拟宿主机","categories":[{"name":"docker","slug":"docker","permalink":"https://www.sakuraus.cn/categories/docker/"},{"name":"docker-machine","slug":"docker/docker-machine","permalink":"https://www.sakuraus.cn/categories/docker/docker-machine/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.sakuraus.cn/tags/docker/"},{"name":"docker-machine","slug":"docker-machine","permalink":"https://www.sakuraus.cn/tags/docker-machine/"}]},{"title":"golang入门之捕捉panic","slug":"go_panic","date":"2018-09-01T16:00:00.000Z","updated":"2018-11-26T13:16:31.666Z","comments":true,"path":"2018/09/02/go_panic/","link":"","permalink":"https://www.sakuraus.cn/2018/09/02/go_panic/","excerpt":"","text":"在其他语言捕捉错误一般是用 try{}catch(E e){}1234567&lt;?phptry&#123; &#125;catch (Exception $e)&#123; // do something...&#125; 捕捉错误很有必要因为你可能不想因为一个小bug导致整个Application 崩溃 捕捉错误记录日志 能有效观察 正式环境的运行情况12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( \"encoding/json\" \"fmt\")type User struct &#123; UserName string `json:\"user_name\"` Age int&#125;func test() string &#123; user1 := &amp;User&#123; UserName: \"zheng\", Age: 22, &#125; defer func() &#123; if err := recover(); err != nil &#123; fmt.Println(err) &#125; &#125;() panic(\"error\") str, _ := json.Marshal(user1) return string(str)&#125;func main() &#123; var data string data = test() fmt.Println(data) var user1 map[string]interface&#123;&#125; err := json.Unmarshal([]byte(data), &amp;user1) if err != nil &#123; fmt.Println(err) return &#125; fmt.Println(\"user2 is\", user1)&#125; 这段代码我正在打包和解析 json 数据 我在 test()函数中手动panic 然后 defer在调用完当前函数后在执行 注意的是 defer必须写在 panic前面不然会报错 手动 panic 是不推荐的坐发可以用记录日志的方式的逻辑处理 panic 主要是语法写错了这类的基础错误 据说go 2.0版本会改进错误处理 golang fans 可以期待下(#^.^#) ,主要是支持泛型了 其他静态语言难找槽点了","categories":[{"name":"golang","slug":"golang","permalink":"https://www.sakuraus.cn/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://www.sakuraus.cn/tags/golang/"}]},{"title":"微服务架构理论-RPC通讯调用","slug":"微服务RPC","date":"2018-09-01T16:00:00.000Z","updated":"2018-11-26T13:16:31.667Z","comments":true,"path":"2018/09/02/微服务RPC/","link":"","permalink":"https://www.sakuraus.cn/2018/09/02/微服务RPC/","excerpt":"","text":"RPCwhat’s Rpc RPC（Remote Procedure Call）—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。 比如说两台服务器A，B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间，不能直接调用，就需要通过网络来表达调用的语义和传达调用的数据，而这种方式就是rpc RPC 的主要功能目标是让构建分布式计算（应用）更容易，在提供强大的远程调用能力时不损失本地调用的语义简洁性。为实现该目标，RPC 框架需提供一种透明调用机制让使用者不必显式的区分本地调用和远程调用。 what need Rpc 分布式部署及微服务当我们的系统访问量增大、业务增多时，我们会发现一台单机运行此系统已经无法承受。此时，我们可以将业务拆分成几个互不关联的应用，分别部署在各自机器上，以划清逻辑并减小压力。 不同技术选型公司业务规模扩大，有可能引入不同的语言，比如A团队要开发CPU密集型的采用java语言，B团队要开发IO密集型的采用PHP，不同语言之间如何通讯 RPC call category 同步调用客户方等待调用执行完成并返回结果。 异步调用客户方调用后不用等待执行结果返回，但依然可以通过回调通知等方式获取返回结果。 若客户方不关心调用返回结果，则变成单向异步调用，单向调用不用返回结果。 异步和同步的区分在于是否等待服务端执行完成并返回结果。 如何调用他人的远程服务 由于各服务部署在不同机器，服务间的调用免不了网络通信过程，服务消费方每调用一个服务都要写一坨网络通信相关的代码，不仅复杂而且极易出错。 如果有一种方式能让我们像调用本地服务一样调用远程服务，而让调用者对网络通信这些细节透明，那么将大大提高生产力，比如服务消费方在执行$client-&gt;getUserInfo()时，实质上调用的是远端的服务。这种方式其实就是RPC（Remote Procedure Call Protocol），在各大互联网公司中被广泛使用，如阿里巴巴的hsf、dubbo（开源）、Facebook的thrift（开源）、Google grpc（开源）。 要让网络通信细节对使用者透明，我们自然需要对通信细节进行封装，我们先看下一个RPC调用的流程： rpc 服务消费方（client）调用以本地调用方式调用服务； client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体； client stub找到服务地址，并将消息发送到服务端； server stub收到消息后进行解码； server stub根据解码结果调用本地的服务； 本地服务执行并将结果返回给server stub； server stub将返回结果打包成消息并发送至消费方； client stub接收到消息，并进行解码； 服务消费方得到最终结果。","categories":[{"name":"MicroService","slug":"MicroService","permalink":"https://www.sakuraus.cn/categories/MicroService/"},{"name":"rpc","slug":"MicroService/rpc","permalink":"https://www.sakuraus.cn/categories/MicroService/rpc/"}],"tags":[{"name":"MicroService","slug":"MicroService","permalink":"https://www.sakuraus.cn/tags/MicroService/"},{"name":"rpc","slug":"rpc","permalink":"https://www.sakuraus.cn/tags/rpc/"}]},{"title":"微服务架构理论-基础与重要部件","slug":"微服务架构重要部件","date":"2018-09-01T16:00:00.000Z","updated":"2018-11-26T13:16:31.667Z","comments":true,"path":"2018/09/02/微服务架构重要部件/","link":"","permalink":"https://www.sakuraus.cn/2018/09/02/微服务架构重要部件/","excerpt":"","text":"内容介绍微服务基础客户端如何访问这些服务（api Gateway） 传统的开发方式，所有的服务都是本地的，UI可以直接调用，现在按功能拆分成独立的服务，跑在独立的运行环境中。客户端UI如何访问他的？后台有N个服务，前台就需要记住管理N个服务，一个服务下线/更新/升级，前台就要重新部署，这明显不服务我们拆分的理念，特别当前台是移动应用的时候，通常业务变化的节奏更快。另外，N个小服务的调用也是一个不小的网络开销。还有一般微服务在系统内部，通常是无状态的，用户登录信息和权限管理最好有一个统一的地方维护管理（OAuth）。 所以，一般在后台N个服务和UI之间一般会一个代理或者叫API Gateway，他的作用包括 •提供统一服务入口，让微服务对前台透明 •聚合后台的服务，节省流量，提升性能 •提供安全，过滤，流控等API管理功能 我的理解其实这个API Gateway可以有很多广义的实现办法，可以是一个软件比如kong，也可以是一个swoole的服务端自己编写的程序。他们最重要的作用是为前台（通常是移动应用）提供后台服务的聚合，提供一个统一的服务出口，解除他们之间的耦合，不过API Gateway也有可能成为单点故障点或者性能的瓶颈。 服务之间如何通信（服务调用） 因为所有的微服务都是独立的进程跑在独立的虚拟机上，所以服务间的通行就是IPC（inter process communication），已经有很多成熟的方案。现在基本最通用的有两种方式。这几种方式，展开来讲都可以写本书，而且大家一般都比较熟悉细节了， 就不展开讲了,简单了解下后面细讲 REST RPC 一般同步调用比较简单，一致性强，但是容易出调用问题，性能体验上也会差些，特别是调用层次多的时候。RESTful和RPC的比较也是一个很有意思的话题。一般REST基于HTTP，更容易实现，更容易被接受，服务端实现技术也更灵活些，各个语言都能支持，同时能跨客户端，对客户端没有特殊的要 求，只要封装了HTTP的SDK就能调用，所以相对使用的广一些。RPC也有自己的优点，传输协议更高效，安全更可控，特别在一个公司内部，如果有统一个的开发规范和统一的服务框架时，他的开发效率优势更明显些。就看各自的技术积累实际条件，自己的选择了。 这么多服务怎么查找（服务发现） 在微服务架构中，一般每一个服务都是有多个拷贝，来做负载均衡。一个服务随时可能下线，也可能应对临时访问压力增加新的服务节点。服务之间如何相互 感知？服务如何管理？这就是服务发现的问题了。一般有两类做法，也各有优缺点。基本都是通过consul等类似技术做服务注册信息的分布式管理。当 服务上线时，服务提供者将自己的服务信息注册到consul（或类似框架），并通过心跳检测健康状态，实时更新链接信息。服务调用者通过consul寻址，根据可定制算法，找到一个服务，还可以将服务信息缓存在本地以提高性能。当服务下线时，consul会发通知给服务客户端。 客户端做：优点是架构简单，扩展灵活，只对服务注册器依赖。缺点是客户端要维护所有调用服务的地址，有技术难度，一般大公司都有成熟的内部框架支持，比如Dubbo。 服务端做：所有服务对于前台调用方透明，一般在小公司在云服务上部署的应用采用的比较多。 service 服务挂了怎么办（服务熔断） 分布式最大的特性就是网络是不可靠 的。通过微服务拆分能降低这个风险，不过如果没有特别的保障，结局肯定是噩梦。比如一个线上故障就是一个很不起眼的数据库修改功能，在访问量上升 时，导致数据库load彪高，影响了所在应用的性能，从而影响所有调用这个应用服务的前台应用。所以当我们的系统是由一系列的服务调用链组成的时候，我们必须确保任一环节出问题都不至于影响整体链路。相应的手段有很多： 重试机制 限流 熔断机制 负载均衡 降级（本地缓存） 微服务重要组成服务注册中心 服务之间需要创建一种服务发现机制，用于帮助服务之间互相感知彼此的存在。服务启动时会将自身的服务信息注册到注册中心，并订阅自己需要消费的服务。 服务注册中心是服务发现的核心。它保存了各个可用服务实例的网络地址（IPAddress和Port）。服务注册中心必须要有高可用性和实时更新功能。上面提到的 consul 就是一个服务注册中心。它提供了服务注册和查询服务信息的REST API。服务通过使用POST请求注册自己的IPAddress和Port。每30秒发送一个PUT请求刷新注册信息。通过DELETE请求注销服务。客户端通过GET请求获取可用的服务实例信息。常见的服务注册中心的有： etcd —– 高可用，分布式，强一致性的，key-value，Kubernetes和Cloud Foundry都是使用了etcd。 consul —–一个用于discovering和configuring的工具。它提供了允许客户端注册和发现服务的API。Consul可以进行服务健康检查，以确定服务的可用性。 zookeeper —— 在分布式应用中被广泛使用，高性能的协调服务。 Apache Zookeeper 最初为Hadoop的一个子项目，但现在是一个顶级项目。 负载均衡 服务高可用的保证手段，为了保证高可用，每一个微服务都需要部署多个服务实例来提供服务。此时客户端进行服务的负载均衡 负载均衡的常见策略 随机把来自网络的请求随机分配给内部中的多个服务器。 轮询每一个来自网络中的请求，轮流分配给内部的服务器，从1到N然后重新开始。此种负载均衡算法适合服务器组内部的服务器都具有相同的配置并且平均服务请求相对均衡的情况。 加权轮询根据服务器的不同处理能力，给每个服务器分配不同的权值，使其能够接受相应权值数的服务请求。例如：服务器A的权值被设计成1，B的权值是3，C的权值是6，则服务器A、B、C将分别接受到10%、30％、60％的服务请求。此种均衡算法能确保高性能的服务器得到更多的使用率，避免低性能的服务器负载过重。 IP Hash这种方式通过生成请求源IP的哈希值，并通过这个哈希值来找到正确的真实服务器。这意味着对于同一主机来说他对应的服务器总是相同。使用这种方式，你不需要保存任何源IP。但是需要注意，这种方式可能导致服务器负载不平衡。 最少连接数客户端的每一次请求服务在服务器停留的时间可能会有较大的差异，随着工作时间加长，如果采用简单的轮循或随机均衡算法，每一台服务器上的连接进程可能会产生极大的不同，并没有达到真正的负载均衡。最少连接数均衡算法对内部中需负载的每一台服务器都有一个数据记录，记录当前该服务器正在处理的连接数量，当有新的服务连接请求时，将把当前请求分配给连接数最少的服务器，使均衡更加符合实际情况，负载更加均衡。此种均衡算法适合长时处理的请求服务，如FTP。 容错 容错，这个词的理解，直面意思就是可以容下错误，不让错误再次扩张，让这个错误产生的影响在一个固定的边界之内，“千里之堤毁于蚁穴”我们用容错的方式就是让这种蚁穴不要变大。那么我们常见的降级，限流，熔断器，超时重试等等都是容错的方法。 在调用服务集群时，如果一个微服务调用异常，如超时，连接异常，网络异常等，则根据容错策略进行服务容错。目前支持的服务容错策略有快速失败，失效切换。如果连续失败多次则直接熔断，不再发起调用。这样可以避免一个服务异常拖垮所有依赖于他的服务。 容错策略 快速失败服务只发起一次调用，失败立即报错。 失效切换服务发起调用，当出现失败后，重试其他服务器。通常用于读操作，但重试会带来更长时间的延迟。重试的次数通常是可以设置的 失败安全失败安全，当服务调用出现异常时，直接忽略。通常用于写入日志等操作。 失败自动恢复当服务调用出现异常时，记录失败请求，定时重发。通常用于消息通知。 广播调用广播调用所有提供者，逐个调用，任何一台失败则失败。通常用于通知所有提供者更新缓存或日志等本地资源信息 熔断 熔断技术可以说是一种“智能化的容错”，当调用满足失败次数，失败比例就会触发熔断器打开，有程序自动切断当前的RPC调用,来防止错误进一步扩大。实现一个熔断器主要是考虑三种模式，关闭，打开，半开。 breaker 我们在处理异常的时候，要根据具体的业务情况来决定处理方式，比如我们调用商品接口，对方只是临时做了降级处理，那么作为网关调用就要切到可替换的服务上来执行或者获取托底数据，给用户友好提示。还有要区分异常的类型，比如依赖的服务崩溃了，这个可能需要花费比较久的时间来解决。也可能是由于服务器负载临时过高导致超时。作为熔断器应该能够甄别这种异常类型，从而根据具体的错误类型调整熔断策略。增加手动设置，在失败的服务恢复时间不确定的情况下，管理员可以手动强制切换熔断状态。最后，熔断器的使用场景是调用可能失败的远程服务程序或者共享资源。如果是本地缓存本地私有资源，使用熔断器则会增加系统的额外开销。还要注意，熔断器不能作为应用程序中业务逻辑的异常处理替代品。 有一些异常比较顽固，突然发生，无法预测，而且很难恢复，并且还会导致级联失败（举个例子，假设一个服务集群的负载非常高，如果这时候集群的一部分挂掉了，还占了很大一部分资源，整个集群都有可能遭殃）。如果我们这时还是不断进行重试的话，结果大多都是失败的。因此，此时我们的应用需要立即进入失败状态(fast-fail)，并采取合适的方法进行恢复。 我们可以用状态机来实现CircuitBreaker，它有以下三种状态： 关闭( Closed )：默认情况下Circuit Breaker是关闭的，此时允许操作执行。CircuitBreaker内部记录着最近失败的次数，如果对应的操作执行失败，次数就会续一次。如果在某个时间段内，失败次数（或者失败比率）达到阈值，CircuitBreaker会转换到开启( Open )状态。在开启状态中，Circuit Breaker会启用一个超时计时器，设这个计时器的目的是给集群相应的时间来恢复故障。当计时器时间到的时候，CircuitBreaker会转换到半开启( Half-Open )状态。 开启( Open )：在此状态下，执行对应的操作将会立即失败并且立即抛出异常。 半开启( Half-Open )：在此状态下，Circuit Breaker会允许执行一定数量的操作。如果所有操作全部成功，CircuitBreaker就会假定故障已经恢复，它就会转换到关闭状态，并且重置失败次数。如果其中 任意一次 操作失败了，Circuit Breaker就会认为故障仍然存在，所以它会转换到开启状态并再次开启计时器（再给系统一些时间使其从失败中恢复） 限流和降级 保证核心服务的稳定性。为了保证核心服务的稳定性，随着访问量的不断增加，需要为系统能够处理的服务数量设置一个极限阀值，超过这个阀值的请求则直接拒绝。同时，为了保证核心服务的可用，可以对否些非核心服务进行降级，通过限制服务的最大访问量进行限流，通过管理控制台对单个微服务进行人工降级 关于降级限流的方法业界都已经有很成熟的方法了，比如FAILBACK机制，限流的方法令牌桶，漏桶，信号量等。这里谈一下我们的一些经验，降级一般都是由统一配置中心的降级开关来实现的，那么当有很多个接口来自同一个提供方，这个提供方的系统或这机器所在机房网络出现了问题，我们就要有一个统一的降级开关，不然就要一个接口一个接口的来降级。也就是要对业务类型有一个大闸刀。还有就是 降级切记暴力降级，什么是暴力降级的，比如把论坛功能降调，结果用户显示一个大白板，我们要实现缓存住一些数据，也就是有托底数据。限流一般分为分布式限流和单机限流，如果实现分布式限流的话就要一个公共的后端存储服务比如redis，在大nginx节点上利用lua读取redis配置信息 API网关 这里说的网关是指API网关，直面意思是将所有API调用统一接入到API网关层，有网关层统一接入和输出。一个网关的基本功能有：统一接入、安全防护、协议适配、流量管控、长短链接支持、容错能力。有了网关之后，各个API服务提供团队可以专注于自己的的业务逻辑处理，而API网关更专注于安全、流量、路由等问题。 超时和重试 超时与重试机制也是容错的一种方法，凡是发生RPC调用的地方，比如读取redis，db，mq等，因为网络故障或者是所依赖的服务故障，长时间不能返回结果，就会导致线程增加，加大cpu负载，甚至导致雪崩。所以对每一个RPC调用都要设置超时时间。 对于强依赖RPC调用资源的情况，还要有重试机制，但是重试的次数建议1-2次，另外如果有重试，那么超时时间就要相应的调小，比如重试1次，那么一共是发生2次调用。如果超时时间配置的是2s，那么客户端就要等待4s才能返回。因此重试+超时的方式，超时时间要调小。 这里也再谈一下一次PRC调用的时间都消耗在哪些环节，一次正常的调用统计的耗时主要包括： ①调用端RPC框架执行时间 + ②网络发送时间 + ③服务端RPC框架执行时间 + ④服务端业务代码时间。调用方和服务方都有各自的性能监控，比如调用方tp99是500ms，服务方tp99是100ms，找了网络组的同事确认网络没有问题。那么时间都花在什么地方了呢，两种原因，客户端调用方，还有一个原因是网络发生TCP重传。所以要注意这两点。","categories":[{"name":"MicroService","slug":"MicroService","permalink":"https://www.sakuraus.cn/categories/MicroService/"}],"tags":[{"name":"MicroService","slug":"MicroService","permalink":"https://www.sakuraus.cn/tags/MicroService/"}]},{"title":"微服务架构—微服务介绍","slug":"微服务详解","date":"2018-09-01T16:00:00.000Z","updated":"2018-11-26T13:16:31.667Z","comments":true,"path":"2018/09/02/微服务详解/","link":"","permalink":"https://www.sakuraus.cn/2018/09/02/微服务详解/","excerpt":"","text":"什么是微服务 在介绍微服务时，首先得先理解什么是微服务，顾名思义，微服务得从两个方面去理解，什么是”微”、什么是”服务”， 微 狭义来讲就是体积小、著名的”2 pizza 团队”很好的诠释了这一解释（2 pizza 团队最早是亚马逊 CEO Bezos提出来的，意思是说单个服务的设计，所有参与人从设计、开发、测试、运维所有人加起来 只需要2个披萨就够了 ）。 而所谓服务，一定要区别于系统，服务一个或者一组相对较小且独立的功能单元，是用户可以感知最小功能集。 微服务的由来 微服务最早由Martin Fowler与James Lewis于2014年共同提出，微服务架构风格是一种使用一套小服务来开发单个应用的方式途径，每个服务运行在自己的进程中，并使用轻量级机制通信，通常是HTTP API，这些服务基于业务能力构建，并能够通过自动化部署机制来独立部署，这些服务使用不同的编程语言实现，以及不同数据存储技术，并保持最低限度的集中式管理。 为什么需要微服务？ 在传统的IT行业软件大多都是各种独立系统的堆砌，这些系统的问题总结来说就是扩展性差，可靠性不高，维护成本高。到后面引入了.服务化，但是，由于 SOA 早期均使用了总线模式，这种总线模式是与某种技术栈强绑定的，这导致很多企业的遗留系统很难对接，切换时间太长，成本太高，新系统稳定性的收敛也需要一些时间。最终 SOA 看起来很美，但却成为了企业级奢侈品，中小公司都望而生畏 什么样的项目适合微服务 单体架构在规模比较小的情况下工作情况良好，但是随着系统规模的扩大，它暴露出来的问题也越来越多，主要有以下几点： 复杂性逐渐变高 比如有的项目有几十万行代码，各个模块之间区别比较模糊，逻辑比较混乱，代码越多复杂性越高，越难解决遇到的问题。 技术债务逐渐上升 公司的人员流动是再正常不过的事情，有的员工在离职之前，疏于代码质量的自我管束，导致留下来很多坑，由于单体项目代码量庞大的惊人，留下的坑很难被发觉，这就给新来的员工带来很大的烦恼，人员流动越大所留下的坑越多，也就是所谓的技术债务越来越多。 阻碍技术创新 比如以前的某个项目使用tp3.2写的，由于各个模块之间有着千丝万缕的联系，代码量大，逻辑不够清楚，如果现在想用tp5来重构这个项目将是非常困难的，付出的成本将非常大，所以更多的时候公司不得不硬着头皮继续使用老的单体架构，这就阻碍了技术的创新。 无法按需伸缩 比如说电影模块是CPU密集型的模块，而订单模块是IO密集型的模块，假如我们要提升订单模块的性能，比如加大内存、增加硬盘，但是由于所有的模块都在一个架构下，因此我们在扩展订单模块的性能时不得不考虑其它模块的因素，因为我们不能因为扩展某个模块的性能而损害其它模块的性能，从而无法按需进行伸缩。 微服务拆分与设计微服务与单体架构区别 单体架构所有的模块全都耦合在一块，代码量大，维护困难，微服务每个模块就相当于一个单独的项目，代码量明显减少，遇到问题也相对来说比较好解决。 单体架构所有的模块都共用一个数据库，存储方式比较单一，微服务每个模块都可以使用不同的存储方式（比如有的用redis，有的用mysql等），数据库也是单个模块对应自己的数据库。 单体架构所有的模块开发所使用的技术一样，微服务每个模块都可以使用不同的开发技术，开发模式更灵活。 微服务与SOA区别 微服务，从本质意义上看，还是 SOA 架构。但内涵有所不同，微服务并不绑定某种特殊的技术，在一个微服务的系统中，可以有 Java 编写的服务，也可以有 php编写的服务，他们是靠Restful架构风格统一成一个系统的。所以微服务本身与具体技术实现无关，扩展性强。 相比传统SOA的服务实现方式，微服务更具有灵活性、可实施性以及可扩展性，其强调的是一种独立测试、独立部署、独立运行的软件架构模式。 微服务拆分与设计 从单体式结构转向微服务架构中会持续碰到服务边界划分的问题：比如，我们有user 服务来提供用户的基础信息，那么用户的头像和图片等是应该单独划分为一个新的service更好还是应该合并到user服务里呢？如果服务的粒度划分的过粗，那就回到了单体式的老路；如果过细，那服务间调用的开销就变得不可忽视了，管理难度也会指数级增加。目前为止还没有一个可以称之为服务边界划分的标准，只能根据不同的业务系统加以调节 拆分的大原则是当一块业务不依赖或极少依赖其它服务，有独立的业务语义，为超过2个的其他服务或客户端提供数据，那么它就应该被拆分成一个独立的服务模块。 微服务设计原则 单一职责原则意思是每个微服务只需要实现自己的业务逻辑就可以了，比如订单管理模块，它只需要处理订单的业务逻辑就可以了，其它的不必考虑。 服务自治原则意思是每个微服务从开发、测试、运维等都是独立的，包括存储的数据库也都是独立的，自己就有一套完整的流程，我们完全可以把它当成一个项目来对待。不必依赖于其它模块。 轻量级通信原则首先是通信的语言非常的轻量，第二，该通信方式需要是跨语言、跨平台的，之所以要跨平台、跨语言就是为了让每个微服务都有足够的独立性，可以不受技术的钳制。 接口明确原则由于微服务之间可能存在着调用关系，为了尽量避免以后由于某个微服务的接口变化而导致其它微服务都做调整，在设计之初就要考虑到所有情况，让接口尽量做的更通用，更灵活，从而尽量避免其它模块也做调整。 微服务优势与缺点特性 每个微服务可独立运行在自己的进程里； 一系列独立运行的微服务共同构建起了整个系统； 每个服务为独立的业务开发，一个微服务一般完成某个特定的功能，比如：订单管理，用户管理等； 微服务之间通过一些轻量级的通信机制进行通信，例如通过REST API或者RPC的方式进行调用。 优点 易于开发和维护由于微服务单个模块就相当于一个项目，开发这个模块我们就只需关心这个模块的逻辑即可，代码量和逻辑复杂度都会降低，从而易于开发和维护。 启动较快这是相对单个微服务来讲的，相比于启动单体架构的整个项目，启动某个模块的服务速度明显是要快很多的。 局部修改容易部署在开发中发现了一个问题，如果是单体架构的话，我们就需要重新发布并启动整个项目，非常耗时间，但是微服务则不同，哪个模块出现了bug我们只需要解决那个模块的bug就可以了，解决完bug之后，我们只需要重启这个模块的服务即可，部署相对简单，不必重启整个项目从而大大节约时间。 技术栈不受限比如订单微服务和电影微服务原来都是用java写的，现在我们想把电影微服务改成php技术，这是完全可以的，而且由于所关注的只是电影的逻辑而已，因此技术更换的成本也就会少很多。 按需伸缩我们上面说了单体架构在想扩展某个模块的性能时不得不考虑到其它模块的性能会不会受影响，对于我们微服务来讲，完全不是问题，电影模块通过什么方式来提升性能不必考虑其它模块的情况。 缺点 运维要求较高对于单体架构来讲，我们只需要维护好这一个项目就可以了，但是对于微服务架构来讲，由于项目是由多个微服务构成的，每个模块出现问题都会造成整个项目运行出现异常，想要知道是哪个模块造成的问题往往是不容易的，因为我们无法一步一步通过debug的方式来跟踪，这就对运维人员提出了很高的要求。 分布式的复杂性对于单体架构来讲，我们可以不使用分布式，但是对于微服务架构来说，分布式几乎是必会用的技术，由于分布式本身的复杂性，导致微服务架构也变得复杂起来。 接口调整成本高比如，用户微服务是要被订单微服务和电影微服务所调用的，一旦用户微服务的接口发生大的变动，那么所有依赖它的微服务都要做相应的调整，由于微服务可能非常多，那么调整接口所造成的成本将会明显提高。 重复劳动对于单体架构来讲，如果某段业务被多个模块所共同使用，我们便可以抽象成一个工具类，被所有模块直接调用，但是微服务却无法这样做，因为这个微服务的工具类是不能被其它微服务所直接调用的，从而我们便不得不在每个微服务上都建这么一个工具类，从而导致代码的重复。","categories":[{"name":"MicroService","slug":"MicroService","permalink":"https://www.sakuraus.cn/categories/MicroService/"}],"tags":[{"name":"MicroService","slug":"MicroService","permalink":"https://www.sakuraus.cn/tags/MicroService/"}]},{"title":"使用github存放markdown所需图片","slug":"markdown-images","date":"2018-09-01T16:00:00.000Z","updated":"2018-11-26T13:16:31.666Z","comments":true,"path":"2018/09/02/markdown-images/","link":"","permalink":"https://www.sakuraus.cn/2018/09/02/markdown-images/","excerpt":"","text":"github存储图片 利用github存储图片，在markdown引用图片链接地址 步骤如下： 自己创建一个公有github仓库 例如 fanyinjiang/markdownImage 先生成 .md 文件 4.点 download 按钮，在地址栏可以复制图片地址，或者在Download按钮上直接右键 “复制链接地址” 拷贝链接地址https://raw.githubusercontent.com/用户名/仓库名/分支/文件名.后缀名 在Markdown中引用图片，![Alt text](图片链接 &quot;optional title&quot;) 插入本地图片 插入本地图片只需要在基础语法的括号中填入图片的位置路径即可，支持绝对路径和相对路径。例如：![avatar](服务器相对路径) 插入网络图片 插入网络图片只需要在基础语法的括号中填入图片的网络链接即可，现在已经有很多免费/收费图床和方便传图的小工具可选。例如：![avatar](http://baidu.com/pic/doge.png)` 把图片存入markdown文件用base64转码工具把图片转成一段字符串，然后把字符串填到基础格式中链接的那个位置。基础用法：![avatar](data:image/png;base64,iVBORw0......)这个时候会发现插入的这一长串字符串会把整个文章分割开，非常影响编写文章时的体验。 如果能够把大段的base64字符串放在文章末尾，然后在文章中通过一个id来调用，文章就不会被分割的这么乱了。就像写论文时的文末的注释和参考文档一样。这个想法可以通过markdown的参考式链接语法来实现。*进阶用法如下：文中引用语法：![avatar][doge]文末存储字符串语法：[doge]:data:image/png;base64,iVBORw0......这个用法不常见，比较野路子。优点是很灵活，不会有链接失效的困扰。缺点是一大团base64的乱码看着不美观。","categories":[{"name":"github","slug":"github","permalink":"https://www.sakuraus.cn/categories/github/"},{"name":"markdown","slug":"github/markdown","permalink":"https://www.sakuraus.cn/categories/github/markdown/"}],"tags":[{"name":"github","slug":"github","permalink":"https://www.sakuraus.cn/tags/github/"},{"name":"markdown","slug":"markdown","permalink":"https://www.sakuraus.cn/tags/markdown/"}]},{"title":"初探srs流媒体-edge边缘服务器","slug":"srs-edge","date":"2018-08-30T16:00:00.000Z","updated":"2018-11-26T13:16:31.667Z","comments":true,"path":"2018/08/31/srs-edge/","link":"","permalink":"https://www.sakuraus.cn/2018/08/31/srs-edge/","excerpt":"","text":"SRS 边缘(edge) 所谓边缘edge服务器，就是边缘直播缓存服务器，配置时指定为remote模式和origin*（指定一个或多个源站IP），这个边缘edge服务器就是源站的缓存了 当用户播放边缘服务器的流时，边缘服务器看有没有缓存，若缓存了就直接将流发给客户端。若没有缓存，则发起一路回源链接，从源站取数据源源不断放到自己的缓存队列。也就是说， 多个客户端连接到边缘时，只有一路回源。这种结构在CDN是最典型的部署结构。譬如北京源站， 在全国32个省每个省都部署了10台服务器，一共就有320台边缘，假设每个省1台边缘服务器都有 2000用户观看，那么就有64万用户，每秒钟集群发送640Gbps数据；而回源链接只有320个， 实现了大规模分发。 边缘edge服务器，实际上是解决大并发问题产生的分布式集群结构。SRS的边缘可以指定多个源站， 在源站出现故障时会自动切换到下一个源站，不影响用户观看，具有最佳的容错性，用户完全不会觉察。 总结:使用edge的好处可以承受更大并发也很容易就能搭建一个srs集群 提高了srs容错.即使srs出了问题在切换源的途中用户也不会察觉 工作模式: 假如我现在有 47.104.10.92:9524 , 127.0.0.1:5685 两台rtmp集群 用户推流只能向一台srs服务器推流如果用户A 在 47.104.10.92:9524 rmtp推流成功,那其他用户也只能通过 47.104.10.92:9524 获取视频流如果使用上 edge 不管 视频流在 47.104.10.92:9524,127.0.0.1:5685 那台srs服务器上edge 会遍历 srs服务器 查询到源站视频流的节点然后建立连接,这样就做了一个很好的中转 文档 edge 注意：优先使用edge，除非知道必须用forward，才使用forward。(forward热备:就是拷贝流到其他srs并不能实现主从)","categories":[{"name":"srs","slug":"srs","permalink":"https://www.sakuraus.cn/categories/srs/"}],"tags":[{"name":"srs","slug":"srs","permalink":"https://www.sakuraus.cn/tags/srs/"}]}]}